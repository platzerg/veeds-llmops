# VEEDS Proofreader ‚Äî LLMOps Stack

Komplettes Setup f√ºr LLM-basiertes Proofreading von YAML-Fahrzeugdaten mit Observability, Evaluation und Load Testing.

## üöÄ Komplette Step-by-Step Anleitung

### **üìã Voraussetzungen pr√ºfen**

```bash
# Node.js Version pr√ºfen (ben√∂tigt: 18+)
node --version

# Docker Status pr√ºfen
docker --version
docker compose ps

# Git Status pr√ºfen
git status
```

docker-compose --profile llm-eval-observability-toolkit up promptfoo-ui

### **üîß Schritt 1: Setup und Dependencies**

#### **1.1 Dependencies installieren**
```bash
npm install
```

#### **1.2 AWS Credentials konfigurieren**
Bearbeiten Sie die `.env`-Datei und f√ºgen Sie Ihre AWS-Zugangsdaten hinzu:

```env
# --- AWS Bedrock ---
AWS_REGION=eu-central-1
AWS_ACCESS_KEY_ID=IHRE_AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY=IHR_AWS_SECRET_ACCESS_KEY
```

#### **1.3 AWS Berechtigung testen**
```bash
# AWS CLI testen (falls installiert)
aws sts get-caller-identity

# Bedrock Zugriff pr√ºfen
aws bedrock list-foundation-models --region eu-central-1
```

#### **1.4 Prompts automatisch in Langfuse erstellen**
```bash
# Prompts automatisch erstellen (empfohlen)
npm run setup:prompts

# Verifikation der erstellten Prompts
npm run setup:prompts:verify

# Setup + Verifikation in einem Schritt
npm run setup:prompts:all
```

**Was passiert:**
- Erstellt `veeds-proofreader` Prompt mit Label `production`
- Erstellt `veeds-proofreader-dev` Prompt mit Label `development`
- L√§dt Prompt-Inhalt aus `eval/prompt.txt`
- Konfiguriert Model-Parameter und Tags automatisch

### **üéØ Schritt 2: Basis-Demo ausf√ºhren**

```bash
npx tsx src/index.ts
```

**Erwartete Ausgabe:**
```
üîç VEEDS Proofreader Demo

--- Test 1: Valid Entry ---
Valid: true
Errors: 0
Time: 1234ms

--- Test 2: Invalid Entry ---
Valid: false
Errors: 3
Time: 1567ms

‚úÖ Done! Check traces at http://localhost:9222
```

**Traces pr√ºfen:** http://localhost:9222 ‚Üí Tracing ‚Üí Traces

### **ü§ñ Schritt 3: Automatische Test-Generierung**

```bash
# Test-Cases generieren (100+ automatische Tests)
npm run generate

# Test-Cases validieren
npx tsx scripts/validate-test-data.ts

# Generierte Tests anzeigen
cat eval/golden_dataset.json | jq '.testCases | length'
```

### **üß™ Schritt 4: Evaluation ausf√ºhren**

```bash
# Basis-Evaluation (alle generierten Tests)
npm run eval

# CI-Mode mit Assertions
npm run eval:assert

# Ergebnisse im Browser anzeigen
npm run eval:view
```

### **‚ö° Schritt 5: Load Testing**

```bash
# Smoke Test (schnell)
npm run test:load:smoke

# Standard Load Test
npm run test:load

# Stress Test (intensiv)
npm run test:load:stress
```

### **üîÑ Schritt 6: Vollst√§ndige CI Pipeline**

```bash
# Komplette Pipeline (Generation + Validation + Evaluation)
npx tsx scripts/ci-test-pipeline.ts

# Pipeline mit Load Tests
npx tsx scripts/ci-test-pipeline.ts --load-tests
```

## üéØ Was Sie alles testen k√∂nnen

### **1. üß† LLM-Qualit√§t testen**

#### **Correctness Testing**
- ‚úÖ Erkennt ung√ºltige materialNumber Formate
- ‚úÖ Erkennt ung√ºltige SI-Einheiten  
- ‚úÖ Erkennt leere Beschreibungen
- ‚úÖ Erkennt min > max Probleme
- ‚úÖ L√§sst g√ºltige Eingaben durch

#### **Edge Case Testing**
- ‚ö†Ô∏è Beschreibung mit exakt 200 Zeichen
- ‚ö†Ô∏è ValueRange mit min = max
- ‚ö†Ô∏è Kleinbuchstaben in materialNumber
- ‚ö†Ô∏è Unicode-Zeichen

#### **Security Testing**
- üõ°Ô∏è Prompt Injection Resistenz
- üõ°Ô∏è YAML Injection Schutz
- üõ°Ô∏è XSS Attempt Handling
- üõ°Ô∏è JSON Injection Schutz

### **2. üìä Performance testen**

#### **Response Time Testing**
```bash
# Einzelner Test
npx tsx -e "
import { proofreadEntry } from './src/proofreader.js';
const start = Date.now();
const result = await proofreadEntry('materialNumber: ABC-12345\ndescription: Test\nunit: mm');
console.log('Time:', Date.now() - start, 'ms');
"
```

#### **Load Testing Szenarien**
- üìà Smoke Test: 1 User, 10s
- üìà Standard: 20 Users, 2min  
- üìà Stress: 200 Users, 6min

#### **Cost Testing**
```bash
# Cost-Analyse in Promptfoo
npm run eval | grep -i cost
```

### **3. üîÑ Regression Testing**

#### **Prompt-√Ñnderungen testen**
```bash
# 1. Prompt in Langfuse √§ndern
# 2. Tests ausf√ºhren
npm run generate:validate

# 3. Vergleiche Ergebnisse
diff eval/validation-report-old.json eval/validation-report.json
```

#### **Model-Vergleich**
```bash
# Verschiedene Claude-Versionen testen
npm run eval:compare
```

### **4. üéõÔ∏è A/B Testing**

#### **Prompt-Versionen vergleichen**
```bash
# 1. Erstelle Prompt v2 in Langfuse
# 2. √Ñndere promptfooconfig.yaml
# 3. Vergleiche
npm run eval:compare
```

### **5. üìà Monitoring & Observability**

#### **Langfuse Dashboard**
- **URL**: http://localhost:9222
- **Metriken**: Cost, Latency, Volume, Quality
- **Traces**: Detaillierte Request-Analyse
- **Scores**: Custom Metrics tracking

### **6. üîß Development Testing**

#### **Einzelne Test-Cases debuggen**
```bash
npx tsx -e "
import { proofreadEntry } from './src/proofreader.js';
const result = await proofreadEntry('materialNumber: INVALID\ndescription: Test\nunit: bananas');
console.log(JSON.stringify(result, null, 2));
"
```

#### **Neue Test-Cases hinzuf√ºgen**
```bash
# 1. Bearbeite scripts/generate-test-data.ts
# 2. Regeneriere Tests
npm run generate
```

## üìä Generierte Reports

- `eval/golden_dataset.json` - Master Test Dataset (100+ Cases)
- `eval/validation-report.json` - Validierungs-Ergebnisse  
- `eval/ci-pipeline-report.json` - CI Pipeline Status
- `promptfoo-output.html` - Evaluation Dashboard

## üö® Troubleshooting

### **AWS Bedrock Fehler**
```bash
# Pr√ºfe Credentials
aws sts get-caller-identity

# Pr√ºfe Bedrock-Zugriff
aws bedrock list-foundation-models --region eu-central-1
```

### **Langfuse Connection Error**
```bash
# Pr√ºfe Docker-Status
docker compose ps

# Pr√ºfe Logs
docker compose logs langfuse-web
```

### **Test-Generierung Fehler**
```bash
# Debug-Modus
DEBUG=1 npm run generate

# Validierungs-Report pr√ºfen
cat eval/validation-report.json | jq '.results[] | select(.passed == false)'
```

### **Automatische Prompt-Erstellung Fehler**
```bash
# Pr√ºfe Langfuse Verbindung
curl http://localhost:9222/api/public/health

# Pr√ºfe API Keys in .env
echo $LANGFUSE_PUBLIC_KEY
echo $LANGFUSE_SECRET_KEY

# Manueller Fallback √ºber UI
# Browser ‚Üí http://localhost:9222 ‚Üí Prompts ‚Üí New Prompt
```
```bash
# Debug-Modus
DEBUG=1 npm run generate

# Validierungs-Report pr√ºfen
cat eval/validation-report.json | jq '.results[] | select(.passed == false)'
```

## ü§ñ Automatische Prompt-Verwaltung

### **Warum automatische Prompt-Erstellung?**
- ‚úÖ **Versionskontrolle**: Prompts sind im Git Repository
- ‚úÖ **Reproduzierbarkeit**: Identische Setups in allen Umgebungen
- ‚úÖ **CI/CD Integration**: Automatisches Deployment von Prompt-√Ñnderungen
- ‚úÖ **Team-Kollaboration**: Prompt-√Ñnderungen √ºber Pull Requests
- ‚úÖ **Backup**: Prompts sind als Dateien gesichert

### **Verf√ºgbare Befehle**
```bash
# Prompts automatisch erstellen/aktualisieren
npm run setup:prompts

# Verifikation der erstellten Prompts
npm run setup:prompts:verify

# Setup + Verifikation in einem Schritt
npm run setup:prompts:all

# Manuell mit erweiterten Optionen
npx tsx scripts/setup-langfuse-http.ts setup
npx tsx scripts/setup-langfuse-http.ts verify
```

### **Was wird erstellt**
- **`veeds-proofreader`** mit Label `production`
- **`veeds-proofreader-dev`** mit Label `development`
- Automatische Model-Konfiguration (Claude 3.5 Sonnet)
- Tags f√ºr bessere Organisation
- Versionierung und Rollback-F√§higkeit

### **Prompt-Anpassung**
Bearbeiten Sie `eval/prompt.txt` und f√ºhren Sie dann aus:
```bash
npm run setup:prompts
```
Die √Ñnderungen werden automatisch als neue Version in Langfuse erstellt.

### **Multi-Environment Support**
```bash
# Verschiedene Umgebungen
LANGFUSE_HOST=http://staging.langfuse.com npm run setup:prompts
LANGFUSE_HOST=http://prod.langfuse.com npm run setup:prompts
```

## üìö Weitere Dokumentation

- **[Test Data Generation Guide](docs/TEST-DATA-GENERATION.md)** - Detaillierte Anleitung zur automatischen Test-Generierung
- **[Langfuse Documentation](https://langfuse.com/docs)** - Offizielle Langfuse Dokumentation
- **[Promptfoo Documentation](https://promptfoo.dev/docs)** - Promptfoo Evaluation Framework
- **[k6 Documentation](https://k6.io/docs)** - Load Testing mit k6

## Stack

| Tool | Zweck | Port |
|------|-------|------|
| **Langfuse v3** | Tracing, Metrics, Prompt Management, LLM-as-Judge, Playground | `:3000` |
| **Promptfoo** | CI/CD Evaluation (YAML-deklarativ, g-eval, llm-rubric) | CLI |
| **k6** | Load & Performance Testing (GraphQL API) | CLI |
| **AWS Bedrock** | LLM Provider (Claude 3.5 Sonnet) | ‚Äî |

## Architektur

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Langfuse v3 (Docker)                     ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  langfuse-web ‚îÄ‚îÄ‚îÄ‚îÄ langfuse-worker                           ‚îÇ
‚îÇ       ‚îÇ                  ‚îÇ                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ  ‚îÇPostgres ‚îÇ    ‚îÇ  ClickHouse    ‚îÇ   Redis    MinIO (S3)     ‚îÇ
‚îÇ  ‚îÇ(Users,  ‚îÇ    ‚îÇ(Traces, Scores,‚îÇ  (Queue)   (Blob Store)  ‚îÇ
‚îÇ  ‚îÇ Prompts)‚îÇ    ‚îÇ Observations)  ‚îÇ                           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                    ‚ñ≤                    ‚ñ≤
         ‚îÇ                    ‚îÇ                    ‚îÇ
    Load Prompt          Push Traces          Push Scores
         ‚îÇ                    ‚îÇ                    ‚îÇ
         ‚ñº                    ‚îÇ                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  VEEDS App      ‚îÇ  ‚îÇ  Promptfoo    ‚îÇ  ‚îÇ    k6           ‚îÇ
‚îÇ  (TypeScript)   ‚îÇ  ‚îÇ  (CI/CD Eval) ‚îÇ  ‚îÇ  (Load Test)    ‚îÇ
‚îÇ                 ‚îÇ  ‚îÇ               ‚îÇ  ‚îÇ                 ‚îÇ
‚îÇ  proofreadEntry ‚îÇ  ‚îÇ  g-eval       ‚îÇ  ‚îÇ  Smoke / Stress ‚îÇ
‚îÇ  ‚Üí Bedrock      ‚îÇ  ‚îÇ  llm-rubric   ‚îÇ  ‚îÇ  p95/p99        ‚îÇ
‚îÇ  ‚Üí Langfuse     ‚îÇ  ‚îÇ  javascript   ‚îÇ  ‚îÇ  Error Rate     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Quick Start

### 1. Setup

```bash
# Secrets generieren und .env erstellen
chmod +x setup.sh
./setup.sh

# Oder manuell
cp .env.example .env
# ‚Üí Secrets in .env anpassen (openssl rand -hex 32)
```

### 2. Langfuse starten

```bash
docker compose up -d

# Warten bis Ready (~2-3 Min)
docker compose logs -f langfuse-web
# ‚Üí "Ready" abwarten

# Status pr√ºfen
docker compose ps
```

### 3. Langfuse konfigurieren

1. Browser ‚Üí **http://localhost:3000**
2. Account erstellen (erster User = Admin)
3. Organisation anlegen (z.B. "VEEDS CORP")
4. Projekt anlegen (z.B. "VEEDS Proofreader")
5. **Settings ‚Üí API Keys** ‚Üí Keys kopieren
6. Keys in `.env` eintragen:
   ```
   LANGFUSE_PUBLIC_KEY=pk-lf-...
   LANGFUSE_SECRET_KEY=sk-lf-...
   ```

### 4. Prompts in Langfuse anlegen

#### **Option A: Automatisch per Code (Empfohlen)**
```bash
# Prompts automatisch erstellen/aktualisieren
npm run setup:prompts

# Verifikation der erstellten Prompts
npm run setup:prompts:verify

# Setup + Verifikation in einem Schritt
npm run setup:prompts:all
```

#### **Option B: Manuell √ºber UI**
1. **Prompts ‚Üí New Prompt**
2. Name: `veeds-proofreader`
3. Inhalt aus `eval/prompt.txt` kopieren
4. Label: `production` setzen

### 5. Dependencies installieren

```bash
npm install
```

### 6. Demo ausf√ºhren

```bash
# AWS Credentials m√ºssen gesetzt sein
npx tsx src/index.ts

# ‚Üí Check Traces in http://localhost:3000
```

## Evaluation

### Automatic Test Data Generation

```bash
# Generate comprehensive test cases automatically
npm run generate

# Generate and validate test cases
npm run generate:validate

# Run full CI pipeline with generation
npx tsx scripts/ci-test-pipeline.ts
```

### Promptfoo

```bash
# Alle Tests ausf√ºhren
npm run eval

# Mit Assertion-Check (f√ºr CI/CD, exit code 1 bei Fehler)
npm run eval:assert

# Ergebnisse im Browser anzeigen
npm run eval:view

# HTML-Report erstellen
npm run eval:compare
```

### k6 Load Test

```bash
# k6 installieren: https://k6.io/docs/get-started/installation/

# Smoke Test (1 VU, 10s)
npm run test:load:smoke

# Default (Ramp to 20 VUs, 2 min)
npm run test:load

# Stress Test (Ramp to 200 VUs)
npm run test:load:stress
```

## Projektstruktur

```
veeds-llmops/
‚îú‚îÄ‚îÄ docker-compose.yml          # Langfuse v3 Full Stack
‚îú‚îÄ‚îÄ .env.example                # Environment Variables Template
‚îú‚îÄ‚îÄ .env                        # Secrets (nicht in Git!)
‚îú‚îÄ‚îÄ setup.sh                    # Secret Generator
‚îú‚îÄ‚îÄ package.json                # Dependencies + Scripts
‚îú‚îÄ‚îÄ tsconfig.json               # TypeScript Config
‚îú‚îÄ‚îÄ promptfooconfig.yaml        # Promptfoo Eval Config
‚îú‚îÄ‚îÄ .gitlab-ci.yml              # CI/CD Pipeline
‚îú‚îÄ‚îÄ .gitignore
‚îÇ
‚îú‚îÄ‚îÄ src/                        # Application Code
‚îÇ   ‚îú‚îÄ‚îÄ index.ts                # Demo / Entry Point
‚îÇ   ‚îú‚îÄ‚îÄ langfuse-client.ts      # Langfuse Singleton
‚îÇ   ‚îî‚îÄ‚îÄ proofreader.ts          # Proofreader mit Tracing
‚îÇ
‚îú‚îÄ‚îÄ scripts/                    # Automation Scripts
‚îÇ   ‚îú‚îÄ‚îÄ generate-test-data.ts   # Automatische Test-Generierung
‚îÇ   ‚îú‚îÄ‚îÄ validate-test-data.ts   # Test-Validierung
‚îÇ   ‚îú‚îÄ‚îÄ ci-test-pipeline.ts     # CI/CD Pipeline
‚îÇ   ‚îú‚îÄ‚îÄ setup-langfuse-http.ts  # Automatisches Prompt-Setup
‚îÇ   ‚îî‚îÄ‚îÄ setup-langfuse-prompts.ts # Alternative Prompt-Setup
‚îÇ
‚îú‚îÄ‚îÄ eval/                       # Evaluation
‚îÇ   ‚îú‚îÄ‚îÄ prompt.txt              # Prompt Template (lokal)
‚îÇ   ‚îú‚îÄ‚îÄ golden_dataset.json     # Golden Dataset (100+ Cases)
‚îÇ   ‚îî‚îÄ‚îÄ results/                # Eval-Ergebnisse
‚îÇ
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ load/
        ‚îî‚îÄ‚îÄ graphql-test.js     # k6 Load Test
```

## N√ºtzliche Befehle

```bash
# --- Setup ---
npm install                     # Dependencies installieren
npm run setup:prompts           # Prompts automatisch in Langfuse erstellen
npm run setup:prompts:verify    # Prompts in Langfuse √ºberpr√ºfen
npm run generate                # Test-Cases generieren
npm run generate:validate       # Generieren + Validieren

# --- Testing ---
npx tsx src/index.ts            # Demo ausf√ºhren
npm run eval                    # Promptfoo Evaluation
npm run eval:assert             # CI-Mode (fail on threshold)
npm run eval:view               # Browser UI
npm run test:load:smoke         # Smoke Test
npm run test:load               # Standard Load Test
npm run test:load:stress        # Stress Test

# --- CI/CD ---
npx tsx scripts/ci-test-pipeline.ts  # Vollst√§ndige Pipeline
npx tsx scripts/validate-test-data.ts # Test-Validierung

# --- Docker ---
docker compose up -d            # Starten
docker compose down             # Stoppen
docker compose logs -f          # Alle Logs
docker compose ps               # Status

# --- Development ---
npm run dev                     # Watch mode
npm run build                   # TypeScript kompilieren
```

## CI/CD Pipeline

| Stage | Job | Trigger | Gate |
|-------|-----|---------|------|
| Generate | `generate-test-data` | Jede MR + Main | ‚ùå Fail = Pipeline blocked |
| Quality | `promptfoo-eval` | Nach Generation | ‚ùå Fail = MR blocked |
| Quality | `promptfoo-compare` | Nightly | ‚ö†Ô∏è Allow failure |
| Performance | `k6-load-test` | Main | ‚ùå Fail = p95 > 3s |
| Performance | `k6-stress-test` | Nightly | ‚ö†Ô∏è Allow failure |

### **Automatische Test-Generierung in CI**
- ü§ñ **100+ Test-Cases** werden automatisch generiert
- üîç **Validierung** gegen echten Proofreader
- üìä **Quality Gates** basierend auf Validierungs-Ergebnissen
- üöÄ **Zero-Maintenance** Testing Pipeline

## Langfuse Features (Self-Hosted)

Alle Features sind seit Juni 2025 unter MIT-Lizenz frei verf√ºgbar:

- ‚úÖ Tracing (End-to-End, Sessions, User Tracking)
- ‚úÖ Metrics Dashboard (Quality, Cost, Latency, Volume)
- ‚úÖ Prompt Management (Versioning, Labels, Rollback)
- ‚úÖ LLM-as-a-Judge (managed Evaluations)
- ‚úÖ Playground (Prompt testen, Model vergleichen)
- ‚úÖ Datasets & Experiments
- ‚úÖ Annotation Queues (Human-in-the-Loop)
- ‚úÖ Cost & Token Tracking
- ‚úÖ OpenTelemetry Integration
- ‚úÖ API (REST + Daily Metrics)

## LLM-as-Judge in Langfuse einrichten

1. **Settings ‚Üí LLM API Keys**
2. Provider hinzuf√ºgen (z.B. AWS Bedrock oder OpenAI)
3. **Evaluations ‚Üí New Evaluator**
4. Template erstellen f√ºr Correctness, Completeness etc.
5. Auf Production-Traces oder Datasets anwenden

---

## üìÅ Detaillierte Projektstruktur

Hier ist eine tiefergehende Aufschl√ºsselung der Verzeichnisse und ihrer Untermodule:

### **1. `src/` (Anwendungslogik)**
*   **`privacy/`**: Beinhaltet den `pii-filter.ts`, der die Schnittstelle zu Microsoft Presidio bildet.
*   **`monitoring/`**: Beinhaltet den `cost-calculator.ts` zur Berechnung der Bedrock-Kosten.
*   **`logging/`**: Konfiguration f√ºr das strukturierte Pino-Logging und Formatierer.
*   **`context-manager.ts`**: Verwalte den asynchronen Speicher (AsyncLocalStorage) f√ºr Trace-IDs.
*   **`proofreader.ts`**: Die Haupt-Pipeline, die alle Module (PII, Bedrock, Cost, Logging) zusammenf√ºhrt.

### **2. `eval/` (Qualit√§t & Daten)**
*   **`results/`**: Hier speichert Promptfoo die HTML/JSON Berichte deiner Tests.
*   **`golden_dataset.json`**: Deine kuratierte Liste an Referenz-Testf√§llen (Ground Truth).
*   **`prompt.txt`**: Das System-Prompt-Template, das als Single-Source-of-Truth dient.

### **3. `scripts/` (Automatisierung & Tools)**
*   **`demo-proofreader.ts`**: Ein schl√ºsselfertiges Skript, um den gesamten Flow live zu zeigen.
*   **`verify-security.ts`**: Deine automatisierte "Defense-Pr√ºfung" gegen Angriffe.
*   **`export-production-traces.ts`**: Das Werkzeug f√ºr den Feedback-Loop (Produktion -> Testdaten).
*   **`setup-langfuse-*.ts`**: Helfer, um Prompts und Konfigurationen in Langfuse einzuspielen.

### **4. `docs/` (Wissensbasis)**
*   **`complete-workflow.md`**: Die "Bibel" f√ºr den gesamten LLMOps-Prozess.
*   **`walkthrough.md`**: Dokumentation der Meilensteine und Verifikations-Ergebnisse.
*   **`architecture/`**: (Optional) Platz f√ºr Deep-Dive Diagramme und Spezifikationen.

### **5. `tests/` & `assertions/`**
*   **`tests/load/`**: k6 Skripte f√ºr Last- und Performance-Tests.
*   **`tests/property-tests/`**: Mathematische Tests f√ºr Randfall-Stabilit√§t.
*   **`assertions/`**: Eigene Pr√ºflogik (JS/Python), um LLM-Antworten fachlich zu validieren.

### **6. `infra/`, `observability/` & `schemas/`**
*   **`infra/presidio/`**: Docker-Konfigurationen und YAML-Settings f√ºr die Anonymisierungs-Engine.
*   **`observability/`**: Setups f√ºr den OpenTelemetry Collector, Grafana Tempo und Dashboards.
*   **`schemas/`**: JSON- und JS-Schemas zur formalen Validierung von Fahrzeugdaten und LLM-Outputs.
*   **`datasets/`**: Verschiedene YAML- und CSV-Testdatenquellen f√ºr gro√üfl√§chige Evaluationen.

### **7. `eval/deepeval/` (Tier 2 Scientific Metrics)**
*   **`bedrock_model.py`**: Python-Adapter f√ºr AWS Bedrock (Claude 3.5).
*   **`test_proofreader.py`**: Definition der Faithfulness- und Relevancy-Tests.
*   **`generate_synthetic_data.py`**: KI-gest√ºtzte Generierung von Test-Cases.
*   **`arena_battle.py`**: A/B Testing Suite f√ºr Modell-Vergleiche.

---

## üî¨ Tier 2: DeepEval Integration

F√ºr fortgeschrittene Szenarien nutzen wir **DeepEval**, um die Qualit√§t unserer LLM-Antworten mit wissenschaftlichen Metriken zu messen.

### **Befehle**
- `npm run eval:deepeval`: F√ºhrt die Python-basierten Metrik-Tests via Docker aus.
- `npm run eval:deepeval:generate`: Erzeugt neue synthetische Test-F√§lle f√ºr dein Modell.
- `npm run eval:deepeval:arena`: Startet eine Arena-Battle zwischen verschiedenen Prompt-Versionen.
- `npm run eval:deepeval:view`: Startet das interaktive DeepEval Dashboard auf Port 8080.

---

## üîÑ Detaillierter LLMOps Workflow & Deep Dive

Dieser Abschnitt bietet eine technische und prozessuale Vertiefung der in **Tier 1 Evolution** implementierten Funktionen.

## üõ°Ô∏è 1. Datenschutz & PII Anonymisierung (`src/privacy/`)
Das System sch√ºtzt personenbezogene Daten (PII) lokal, bevor sie an Cloud-Provider wie AWS Bedrock oder Langfuse √ºbertragen werden.

*   **Technologie**: Nutzt **Microsoft Presidio** via Docker.
*   **Ablauf**:
    1.  Eingabetext wird an `pii-filter.ts` √ºbergeben.
    2.  Der **Presidio Analyzer** identifiziert Entit√§ten (Namen, E-Mails, Telefonnummern).
    3.  Der **Presidio Anonymizer** ersetzt diese durch Platzhalter (z.B. `<PERSON>`, `<EMAIL_ADDRESS>`).
    4.  Erst der anonymisierte Text wird f√ºr den LLM-Prompt und das Langfuse-Tracing verwendet.
*   **Skript**: `npm run test:pii` (verifiziert die Redaktion).

## üí∏ 2. Cost Tracking & Wirtschaftlichkeit (`src/monitoring/`)
Jeder LLM-Call wird finanziell √ºberwacht, um die Profitabilit√§t des Dienstes sicherzustellen.

*   **Implementierung**: `cost-calculator.ts` enth√§lt die aktuellen Preislisten f√ºr **Claude 3.5 Sonnet** (Input/Output Tokens).
*   **Integration**: Die Kosten werden in `proofreader.ts` berechnet und:
    1.  An **Langfuse** als `metadata.cost` gesendet (f√ºr Dashboards).
    2.  Im strukturierten **Pino-Log** (`logBedrock`) erfasst.
    3.  Dem Endnutzer im Resultat-Objekt zur√ºckgegeben.
*   **Vorteil**: Du siehst in Langfuse sofort, welche Query wie viel Cent gekostet hat.

## üîÑ 3. Continuous Improvement Feedback-Loop
Wir nutzen echte Produktionsdaten, um unsere Testabdeckung automatisch zu verbessern.

*   **Mechanik**: `scripts/export-production-traces.ts`
*   **Workflow**:
    1.  Das Skript zieht Traces aus Langfuse, die vom System (oder manuell) als "verbesserungsw√ºrdig" markiert wurden.
    2.  Diese werden automatisch in das Format von `eval/golden_dataset.json` konvertiert.
    3.  Beim n√§chsten `npm run eval` wird sichergestellt, dass das System aus diesen realen Fehlern gelernt hat (Regression-Testing).

## üõ°Ô∏è 4. Security Verification Suite (`scripts/verify-security.ts`)
Ein spezialisiertes Test-Toolkit, das √ºber normale Unit-Tests hinausgeht.

*   **npm run test:verify**: F√ºhrt eine Reihe von "Adversarial Attacks" aus:
    - **Prompt Injection**: Versucht, das System zu zwingen, seine Anweisungen zu ignorieren.
    - **PII Leakage**: Testet, ob das System sensible Daten im Output ausgibt.
    - **Harmful Content**: Stellt sicher, dass keine gef√§hrlichen Antworten generiert werden.

---

## üî¨ Tier 2+: Advanced DeepEval & Automation

F√ºr ultimative Kontrolle und Automatisierung bietet der Stack fortgeschrittene Python-basierte Werkzeuge:

### **1. DeepEval Arena (Befehl: `npm run eval:deepeval:arena`)**
Erm√∂glicht den **A/B Vergleich** von zwei verschiedenen Prompts oder Modellen.
*   **Workflow**: Schickt denselben Input an beide Varianten.
*   **Judge**: Claude 3.5 bewertet beide Antworten mathematisch auf Relevanz.
*   **Visualisierung**: Die Ergebnisse werden als direkt genutzte Scores nach Langfuse √ºbertragen.

### **2. Langfuse Auto-Scorer (Befehl: `npm run automation:score`)**
Ein Hintergrund-Skript, das Traces automatisch "benotet".
*   **Funktion**: Scannt Traces auf Fehlermuster (z.B. `Valid: false`).
*   **Output**: Setzt automatisiert Scores (0 f√ºr Fehler, 1 f√ºr Erfolg) in Langfuse.
*   **Vorteil**: Massive Zeitersparnis beim manuellen Review von tausenden Traces.

### **3. Prompt-as-Code Sync (Befehl: `npm run prompt:sync`)**
H√§lt dein Repository und dein Langfuse-Dashboard synchron.
*   **Funktion**: Pusht den Inhalt von `eval/prompt.txt` als neue Version in die Langfuse Prompt Registry.
*   **Vorteil**: Erm√∂glicht echtes Version-Control f√ºr LLM-Prompts in Git.

---

## üõ†Ô∏è Detaillierte NPM Skript-Referenz

| Bereich | Befehl | Aktion / Kommando | Technischer Zweck |
| :--- | :--- | :--- | :--- |
| **Setup** | `npm run setup` | `./setup.sh` | Secrets & .env Initialisierung. |
| | `npm run up` | `docker up -d` | Startet den kompletten AI Stack. |
| **Dev** | `npm run demo` | `demo-proofreader.ts` | E2E Durchlauf (PII -> Bedrock -> Cost). |
| **Eval** | `npm run eval` | `promptfoo eval` | Logische Validierung (Tier 1). |
| | `npm run eval:deepeval` | `deepeval run` | Wissenschaftliche Metriken (Tier 2). |
| | `npm run eval:deepeval:generate`| `synthetic_data.py` | **Synthetische Daten** generieren. |
| | `npm run eval:deepeval:arena`| `arena_battle.py` | **A/B Testing** zweier Prompts. |
| | `npm run eval:deepeval:view` | `deepeval dashboard`| **DeepEval Dashboard** (Port 8080). |
| **Automation** | `npm run prompt:sync` | `prompt-sync.py` | **Git-to-Langfuse** Prompt Sync. |
| | `npm run automation:score` | `auto-scorer.py` | **Automatisches Grading** in Langfuse. |
| **Security** | `npm run redteam` | `promptfoo redteam` | Automatisierte Sicherheits-Angriffe. |

---

## üìñ Glossar & Fachbegriffe

*   **Golden Dataset**: Ein handverlesener Satz von Testf√§llen (Ground Truth).
*   **PII (Privacy)**: Schutz personenbezogener Daten vor dem Cloud-Versand.
*   **LLM-as-a-Judge**: Ein starkes Modell bewertet die Antwort eines anderen Modells.
*   **Auto-Scoring**: Automatische Qualit√§ts-Bewertung basierend auf Log-Patterns.

---

## üöÄ Best-Practice Workflows

### **Szenario A: Prompt-√Ñnderung & Synchronisierung**
1.  Status-Quo in Langfuse pr√ºfen.
2.  Lokalen Prompt in `eval/prompt.txt` editieren.
3.  `npm run prompt:sync` ausf√ºhren (neue Version in Langfuse).
4.  `npm run eval` zur Absicherung der Regressionen.

### **Szenario B: Sicherheitsl√ºcke schlie√üen**
1.  Problem in `scripts/verify-security.ts` als neuen Test-Case erg√§nzen.
2.  L√ºcke reproduzieren (Test schl√§gt fehl).
3.  Prompt verbessern, bis `npm run test:verify` besteht.

### **Szenario C: A/B Testing (Arena Battle)**
1.  Zwei Prompt-Ideen in `eval/deepeval/arena_battle.py` definieren.
2.  `npm run eval:deepeval:arena` ausf√ºhren.
3.  In Langfuse die Scores vergleichen und den "Winner" zum Standard machen.

---
¬© 2026 VEEDS CORP - Advanced LLMOps Infrastructure
