# VEEDS Proofreader â€” LLMOps Stack

Komplettes Setup fÃ¼r LLM-basiertes Proofreading von YAML-Fahrzeugdaten mit Observability, Evaluation und Load Testing.

## ğŸš€ Komplette Step-by-Step Anleitung

### **ğŸ“‹ Voraussetzungen prÃ¼fen**

```bash
# Node.js Version prÃ¼fen (benÃ¶tigt: 18+)
node --version

# Docker Status prÃ¼fen
docker --version
docker compose ps

# Git Status prÃ¼fen
git status
```

docker-compose --profile llm-eval-observability-toolkit up promptfoo-ui

### **ğŸ”§ Schritt 1: Setup und Dependencies**

#### **1.1 Dependencies installieren**
```bash
npm install
```

#### **1.2 AWS Credentials konfigurieren**
Bearbeiten Sie die `.env`-Datei und fÃ¼gen Sie Ihre AWS-Zugangsdaten hinzu:

```env
# --- AWS Bedrock ---
AWS_REGION=eu-central-1
AWS_ACCESS_KEY_ID=IHRE_AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY=IHR_AWS_SECRET_ACCESS_KEY
```

#### **1.3 AWS Berechtigung testen**
```bash
# AWS CLI testen (falls installiert)
aws sts get-caller-identity

# Bedrock Zugriff prÃ¼fen
aws bedrock list-foundation-models --region eu-central-1
```

#### **1.4 Prompts automatisch in Langfuse erstellen**
```bash
# Prompts automatisch erstellen (empfohlen)
npm run setup:prompts

# Verifikation der erstellten Prompts
npm run setup:prompts:verify

# Setup + Verifikation in einem Schritt
npm run setup:prompts:all
```

**Was passiert:**
- Erstellt `veeds-proofreader` Prompt mit Label `production`
- Erstellt `veeds-proofreader-dev` Prompt mit Label `development`
- LÃ¤dt Prompt-Inhalt aus `eval/prompt.txt`
- Konfiguriert Model-Parameter und Tags automatisch

### **ğŸ¯ Schritt 2: Basis-Demo ausfÃ¼hren**

```bash
npx tsx src/index.ts
```

**Erwartete Ausgabe:**
```
ğŸ” VEEDS Proofreader Demo

--- Test 1: Valid Entry ---
Valid: true
Errors: 0
Time: 1234ms

--- Test 2: Invalid Entry ---
Valid: false
Errors: 3
Time: 1567ms

âœ… Done! Check traces at http://localhost:9222
```

**Traces prÃ¼fen:** http://localhost:9222 â†’ Tracing â†’ Traces

### **ğŸ¤– Schritt 3: Automatische Test-Generierung**

```bash
# Test-Cases generieren (100+ automatische Tests)
npm run generate

# Test-Cases validieren
npx tsx scripts/validate-test-data.ts

# Generierte Tests anzeigen
cat eval/golden_dataset.json | jq '.testCases | length'
```

### **ğŸ§ª Schritt 4: Evaluation ausfÃ¼hren**

```bash
# Basis-Evaluation (alle generierten Tests)
npm run eval

# CI-Mode mit Assertions
npm run eval:assert

# Ergebnisse im Browser anzeigen
npm run eval:view
```

### **âš¡ Schritt 5: Load Testing**

```bash
# Smoke Test (schnell)
npm run test:load:smoke

# Standard Load Test
npm run test:load

# Stress Test (intensiv)
npm run test:load:stress
```

### **ğŸ”„ Schritt 6: VollstÃ¤ndige CI Pipeline**

```bash
# Komplette Pipeline (Generation + Validation + Evaluation)
npx tsx scripts/ci-test-pipeline.ts

# Pipeline mit Load Tests
npx tsx scripts/ci-test-pipeline.ts --load-tests
```

## ğŸ¯ Was Sie alles testen kÃ¶nnen

### **1. ğŸ§  LLM-QualitÃ¤t testen**

#### **Correctness Testing**
- âœ… Erkennt ungÃ¼ltige materialNumber Formate
- âœ… Erkennt ungÃ¼ltige SI-Einheiten  
- âœ… Erkennt leere Beschreibungen
- âœ… Erkennt min > max Probleme
- âœ… LÃ¤sst gÃ¼ltige Eingaben durch

#### **Edge Case Testing**
- âš ï¸ Beschreibung mit exakt 200 Zeichen
- âš ï¸ ValueRange mit min = max
- âš ï¸ Kleinbuchstaben in materialNumber
- âš ï¸ Unicode-Zeichen

#### **Security Testing**
- ğŸ›¡ï¸ Prompt Injection Resistenz
- ğŸ›¡ï¸ YAML Injection Schutz
- ğŸ›¡ï¸ XSS Attempt Handling
- ğŸ›¡ï¸ JSON Injection Schutz

### **2. ğŸ“Š Performance testen**

#### **Response Time Testing**
```bash
# Einzelner Test
npx tsx -e "
import { proofreadEntry } from './src/proofreader.js';
const start = Date.now();
const result = await proofreadEntry('materialNumber: ABC-12345\ndescription: Test\nunit: mm');
console.log('Time:', Date.now() - start, 'ms');
"
```

#### **Load Testing Szenarien**
- ğŸ“ˆ Smoke Test: 1 User, 10s
- ğŸ“ˆ Standard: 20 Users, 2min  
- ğŸ“ˆ Stress: 200 Users, 6min

#### **Cost Testing**
```bash
# Cost-Analyse in Promptfoo
npm run eval | grep -i cost
```

### **3. ğŸ”„ Regression Testing**

#### **Prompt-Ã„nderungen testen**
```bash
# 1. Prompt in Langfuse Ã¤ndern
# 2. Tests ausfÃ¼hren
npm run generate:validate

# 3. Vergleiche Ergebnisse
diff eval/validation-report-old.json eval/validation-report.json
```

#### **Model-Vergleich**
```bash
# Verschiedene Claude-Versionen testen
npm run eval:compare
```

### **4. ğŸ›ï¸ A/B Testing**

#### **Prompt-Versionen vergleichen**
```bash
# 1. Erstelle Prompt v2 in Langfuse
# 2. Ã„ndere promptfooconfig.yaml
# 3. Vergleiche
npm run eval:compare
```

### **5. ğŸ“ˆ Monitoring & Observability**

#### **Langfuse Dashboard**
- **URL**: http://localhost:9222
- **Metriken**: Cost, Latency, Volume, Quality
- **Traces**: Detaillierte Request-Analyse
- **Scores**: Custom Metrics tracking

### **6. ğŸ”§ Development Testing**

#### **Einzelne Test-Cases debuggen**
```bash
npx tsx -e "
import { proofreadEntry } from './src/proofreader.js';
const result = await proofreadEntry('materialNumber: INVALID\ndescription: Test\nunit: bananas');
console.log(JSON.stringify(result, null, 2));
"
```

#### **Neue Test-Cases hinzufÃ¼gen**
```bash
# 1. Bearbeite scripts/generate-test-data.ts
# 2. Regeneriere Tests
npm run generate
```

## ğŸ“Š Generierte Reports

- `eval/golden_dataset.json` - Master Test Dataset (100+ Cases)
- `eval/validation-report.json` - Validierungs-Ergebnisse  
- `eval/ci-pipeline-report.json` - CI Pipeline Status
- `promptfoo-output.html` - Evaluation Dashboard

## ğŸš¨ Troubleshooting

### **AWS Bedrock Fehler**
```bash
# PrÃ¼fe Credentials
aws sts get-caller-identity

# PrÃ¼fe Bedrock-Zugriff
aws bedrock list-foundation-models --region eu-central-1
```

### **Langfuse Connection Error**
```bash
# PrÃ¼fe Docker-Status
docker compose ps

# PrÃ¼fe Logs
docker compose logs langfuse-web
```

### **Test-Generierung Fehler**
```bash
# Debug-Modus
DEBUG=1 npm run generate

# Validierungs-Report prÃ¼fen
cat eval/validation-report.json | jq '.results[] | select(.passed == false)'
```

### **Automatische Prompt-Erstellung Fehler**
```bash
# PrÃ¼fe Langfuse Verbindung
curl http://localhost:9222/api/public/health

# PrÃ¼fe API Keys in .env
echo $LANGFUSE_PUBLIC_KEY
echo $LANGFUSE_SECRET_KEY

# Manueller Fallback Ã¼ber UI
# Browser â†’ http://localhost:9222 â†’ Prompts â†’ New Prompt
```
```bash
# Debug-Modus
DEBUG=1 npm run generate

# Validierungs-Report prÃ¼fen
cat eval/validation-report.json | jq '.results[] | select(.passed == false)'
```

## ğŸ¤– Automatische Prompt-Verwaltung

### **Warum automatische Prompt-Erstellung?**
- âœ… **Versionskontrolle**: Prompts sind im Git Repository
- âœ… **Reproduzierbarkeit**: Identische Setups in allen Umgebungen
- âœ… **CI/CD Integration**: Automatisches Deployment von Prompt-Ã„nderungen
- âœ… **Team-Kollaboration**: Prompt-Ã„nderungen Ã¼ber Pull Requests
- âœ… **Backup**: Prompts sind als Dateien gesichert

### **VerfÃ¼gbare Befehle**
```bash
# Prompts automatisch erstellen/aktualisieren
npm run setup:prompts

# Verifikation der erstellten Prompts
npm run setup:prompts:verify

# Setup + Verifikation in einem Schritt
npm run setup:prompts:all

# Manuell mit erweiterten Optionen
npx tsx scripts/setup-langfuse-http.ts setup
npx tsx scripts/setup-langfuse-http.ts verify
```

### **Was wird erstellt**
- **`veeds-proofreader`** mit Label `production`
- **`veeds-proofreader-dev`** mit Label `development`
- Automatische Model-Konfiguration (Claude 3.5 Sonnet)
- Tags fÃ¼r bessere Organisation
- Versionierung und Rollback-FÃ¤higkeit

### **Prompt-Anpassung**
Bearbeiten Sie `eval/prompt.txt` und fÃ¼hren Sie dann aus:
```bash
npm run setup:prompts
```
Die Ã„nderungen werden automatisch als neue Version in Langfuse erstellt.

### **Multi-Environment Support**
```bash
# Verschiedene Umgebungen
LANGFUSE_HOST=http://staging.langfuse.com npm run setup:prompts
LANGFUSE_HOST=http://prod.langfuse.com npm run setup:prompts
```

## ğŸ“š Weitere Dokumentation

- **[Test Data Generation Guide](docs/TEST-DATA-GENERATION.md)** - Detaillierte Anleitung zur automatischen Test-Generierung
- **[Langfuse Documentation](https://langfuse.com/docs)** - Offizielle Langfuse Dokumentation
- **[Promptfoo Documentation](https://promptfoo.dev/docs)** - Promptfoo Evaluation Framework
- **[k6 Documentation](https://k6.io/docs)** - Load Testing mit k6

## Stack

| Tool | Zweck | Port |
|------|-------|------|
| **Langfuse v3** | Tracing, Metrics, Prompt Management, LLM-as-Judge, Playground | `:3000` |
| **Promptfoo** | CI/CD Evaluation (YAML-deklarativ, g-eval, llm-rubric) | CLI |
| **k6** | Load & Performance Testing (GraphQL API) | CLI |
| **AWS Bedrock** | LLM Provider (Claude 3.5 Sonnet) | â€” |

## Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Langfuse v3 (Docker)                     â”‚
â”‚                                                              â”‚
â”‚  langfuse-web â”€â”€â”€â”€ langfuse-worker                           â”‚
â”‚       â”‚                  â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚Postgres â”‚    â”‚  ClickHouse    â”‚   Redis    MinIO (S3)     â”‚
â”‚  â”‚(Users,  â”‚    â”‚(Traces, Scores,â”‚  (Queue)   (Blob Store)  â”‚
â”‚  â”‚ Prompts)â”‚    â”‚ Observations)  â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â–²                    â–²
         â”‚                    â”‚                    â”‚
    Load Prompt          Push Traces          Push Scores
         â”‚                    â”‚                    â”‚
         â–¼                    â”‚                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VEEDS App      â”‚  â”‚  Promptfoo    â”‚  â”‚    k6           â”‚
â”‚  (TypeScript)   â”‚  â”‚  (CI/CD Eval) â”‚  â”‚  (Load Test)    â”‚
â”‚                 â”‚  â”‚               â”‚  â”‚                 â”‚
â”‚  proofreadEntry â”‚  â”‚  g-eval       â”‚  â”‚  Smoke / Stress â”‚
â”‚  â†’ Bedrock      â”‚  â”‚  llm-rubric   â”‚  â”‚  p95/p99        â”‚
â”‚  â†’ Langfuse     â”‚  â”‚  javascript   â”‚  â”‚  Error Rate     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### 1. Setup

```bash
# Secrets generieren und .env erstellen
chmod +x setup.sh
./setup.sh

# Oder manuell
cp .env.example .env
# â†’ Secrets in .env anpassen (openssl rand -hex 32)
```

### 2. Langfuse starten

```bash
docker compose up -d

# Warten bis Ready (~2-3 Min)
docker compose logs -f langfuse-web
# â†’ "Ready" abwarten

# Status prÃ¼fen
docker compose ps
```

### 3. Langfuse konfigurieren

1. Browser â†’ **http://localhost:3000**
2. Account erstellen (erster User = Admin)
3. Organisation anlegen (z.B. "VEEDS CORP")
4. Projekt anlegen (z.B. "VEEDS Proofreader")
5. **Settings â†’ API Keys** â†’ Keys kopieren
6. Keys in `.env` eintragen:
   ```
   LANGFUSE_PUBLIC_KEY=pk-lf-...
   LANGFUSE_SECRET_KEY=sk-lf-...
   ```

### 4. Prompts in Langfuse anlegen

#### **Option A: Automatisch per Code (Empfohlen)**
```bash
# Prompts automatisch erstellen/aktualisieren
npm run setup:prompts

# Verifikation der erstellten Prompts
npm run setup:prompts:verify

# Setup + Verifikation in einem Schritt
npm run setup:prompts:all
```

#### **Option B: Manuell Ã¼ber UI**
1. **Prompts â†’ New Prompt**
2. Name: `veeds-proofreader`
3. Inhalt aus `eval/prompt.txt` kopieren
4. Label: `production` setzen

### 5. Dependencies installieren

```bash
npm install
```

### 6. Demo ausfÃ¼hren

```bash
# AWS Credentials mÃ¼ssen gesetzt sein
npx tsx src/index.ts

# â†’ Check Traces in http://localhost:3000
```

## Evaluation

### Automatic Test Data Generation

```bash
# Generate comprehensive test cases automatically
npm run generate

# Generate and validate test cases
npm run generate:validate

# Run full CI pipeline with generation
npx tsx scripts/ci-test-pipeline.ts
```

### Promptfoo

```bash
# Alle Tests ausfÃ¼hren
npm run eval

# Mit Assertion-Check (fÃ¼r CI/CD, exit code 1 bei Fehler)
npm run eval:assert

# Ergebnisse im Browser anzeigen
npm run eval:view

# HTML-Report erstellen
npm run eval:compare
```

### k6 Load Test

```bash
# k6 installieren: https://k6.io/docs/get-started/installation/

# Smoke Test (1 VU, 10s)
npm run test:load:smoke

# Default (Ramp to 20 VUs, 2 min)
npm run test:load

# Stress Test (Ramp to 200 VUs)
npm run test:load:stress
```

## Projektstruktur

```
veeds-llmops/
â”œâ”€â”€ docker-compose.yml          # Langfuse v3 Full Stack
â”œâ”€â”€ .env.example                # Environment Variables Template
â”œâ”€â”€ .env                        # Secrets (nicht in Git!)
â”œâ”€â”€ setup.sh                    # Secret Generator
â”œâ”€â”€ package.json                # Dependencies + Scripts
â”œâ”€â”€ tsconfig.json               # TypeScript Config
â”œâ”€â”€ promptfooconfig.yaml        # Promptfoo Eval Config
â”œâ”€â”€ .gitlab-ci.yml              # CI/CD Pipeline
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ src/                        # Application Code
â”‚   â”œâ”€â”€ index.ts                # Demo / Entry Point
â”‚   â”œâ”€â”€ langfuse-client.ts      # Langfuse Singleton
â”‚   â””â”€â”€ proofreader.ts          # Proofreader mit Tracing
â”‚
â”œâ”€â”€ scripts/                    # Automation Scripts
â”‚   â”œâ”€â”€ generate-test-data.ts   # Automatische Test-Generierung
â”‚   â”œâ”€â”€ validate-test-data.ts   # Test-Validierung
â”‚   â”œâ”€â”€ ci-test-pipeline.ts     # CI/CD Pipeline
â”‚   â”œâ”€â”€ setup-langfuse-http.ts  # Automatisches Prompt-Setup
â”‚   â””â”€â”€ setup-langfuse-prompts.ts # Alternative Prompt-Setup
â”‚
â”œâ”€â”€ eval/                       # Evaluation
â”‚   â”œâ”€â”€ prompt.txt              # Prompt Template (lokal)
â”‚   â”œâ”€â”€ golden_dataset.json     # Golden Dataset (100+ Cases)
â”‚   â””â”€â”€ results/                # Eval-Ergebnisse
â”‚
â””â”€â”€ tests/
    â””â”€â”€ load/
        â””â”€â”€ graphql-test.js     # k6 Load Test
```

## NÃ¼tzliche Befehle

```bash
# --- Setup ---
npm install                     # Dependencies installieren
npm run setup:prompts           # Prompts automatisch in Langfuse erstellen
npm run setup:prompts:verify    # Prompts in Langfuse Ã¼berprÃ¼fen
npm run generate                # Test-Cases generieren
npm run generate:validate       # Generieren + Validieren

# --- Testing ---
npx tsx src/index.ts            # Demo ausfÃ¼hren
npm run eval                    # Promptfoo Evaluation
npm run eval:assert             # CI-Mode (fail on threshold)
npm run eval:view               # Browser UI
npm run test:load:smoke         # Smoke Test
npm run test:load               # Standard Load Test
npm run test:load:stress        # Stress Test

# --- CI/CD ---
npx tsx scripts/ci-test-pipeline.ts  # VollstÃ¤ndige Pipeline
npx tsx scripts/validate-test-data.ts # Test-Validierung

# --- Docker ---
docker compose up -d            # Starten
docker compose down             # Stoppen
docker compose logs -f          # Alle Logs
docker compose ps               # Status

# --- Development ---
npm run dev                     # Watch mode
npm run build                   # TypeScript kompilieren
```

## CI/CD Pipeline

| Stage | Job | Trigger | Gate |
|-------|-----|---------|------|
| Generate | `generate-test-data` | Jede MR + Main | âŒ Fail = Pipeline blocked |
| Quality | `promptfoo-eval` | Nach Generation | âŒ Fail = MR blocked |
| Quality | `promptfoo-compare` | Nightly | âš ï¸ Allow failure |
| Performance | `k6-load-test` | Main | âŒ Fail = p95 > 3s |
| Performance | `k6-stress-test` | Nightly | âš ï¸ Allow failure |

### **Automatische Test-Generierung in CI**
- ğŸ¤– **100+ Test-Cases** werden automatisch generiert
- ğŸ” **Validierung** gegen echten Proofreader
- ğŸ“Š **Quality Gates** basierend auf Validierungs-Ergebnissen
- ğŸš€ **Zero-Maintenance** Testing Pipeline

## Langfuse Features (Self-Hosted)

Alle Features sind seit Juni 2025 unter MIT-Lizenz frei verfÃ¼gbar:

- âœ… Tracing (End-to-End, Sessions, User Tracking)
- âœ… Metrics Dashboard (Quality, Cost, Latency, Volume)
- âœ… Prompt Management (Versioning, Labels, Rollback)
- âœ… LLM-as-a-Judge (managed Evaluations)
- âœ… Playground (Prompt testen, Model vergleichen)
- âœ… Datasets & Experiments
- âœ… Annotation Queues (Human-in-the-Loop)
- âœ… Cost & Token Tracking
- âœ… OpenTelemetry Integration
- âœ… API (REST + Daily Metrics)

## LLM-as-Judge in Langfuse einrichten

1. **Settings â†’ LLM API Keys**
2. Provider hinzufÃ¼gen (z.B. AWS Bedrock oder OpenAI)
3. **Evaluations â†’ New Evaluator**
4. Template erstellen fÃ¼r Correctness, Completeness etc.
5. Auf Production-Traces oder Datasets anwenden
