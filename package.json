{
  "name": "veeds-llmops",
  "version": "1.0.0",
  "description": "VEEDS Proofreader - LLMOps Stack (Langfuse + Promptfoo + k6)",
  "private": true,
  "type": "module",
  "main": "dist/agentcore-evaluations-example.js",
  "scripts": {
    "setup": "./setup.sh",
    "up": "docker compose up -d",
    "down": "docker compose down",
    "logs": "docker compose logs -f",
    "logs:web": "docker compose logs -f langfuse-web",
    "logs:worker": "docker compose logs -f langfuse-worker",
    "status": "docker compose ps",
    "health": "./scripts/health-check.sh",
    "seed": "npx tsx scripts/seed-langfuse.ts",
    "eval:generate": "npx tsx eval/generate-promptfoo-tests.ts",
    "eval": "npm run eval:generate && npx promptfoo eval -c promptfooconfig.yaml",
    "eval:view": "npx promptfoo view --port 3210",
    "demo": "npx tsx scripts/demo-proofreader.ts",
    "test:verify": "npx tsx scripts/verify-security.ts",
    "test:pii": "npx tsx scripts/debug-pii.ts",
    "eval:assert": "npm run eval:generate && npx promptfoo eval -c promptfooconfig.yaml --assert",
    "eval:compare": "npm run eval:generate && npx promptfoo eval -c promptfooconfig.yaml --output eval/results/latest.html && open eval/results/latest.html",
    "eval:push": "npx tsx scripts/push-scores-to-langfuse.ts eval/results/latest.json",
    "eval:deepeval": "docker compose --profile deepeval run --rm deepeval",
    "eval:deepeval:generate": "docker compose --profile deepeval run --rm deepeval sh -c \"pip install -r eval/deepeval/requirements.txt && python eval/deepeval/generate_synthetic_data.py\"",
    "eval:deepeval:arena": "docker compose --profile deepeval run --rm deepeval sh -c \"pip install -r eval/deepeval/requirements.txt && python eval/deepeval/arena_battle.py\"",
    "eval:deepeval:view": "docker compose --profile deepeval-ui up deepeval-ui",
    "eval:full": "npm run eval && npm run eval:push",
    "automation:score": "python scripts/auto-scorer.py",
    "dataset:upload": "npx tsx eval/upload-dataset-to-langfuse.ts",
    "dataset:export": "npx tsx scripts/export-production-traces.ts",
    "test": "node --experimental-vm-modules ./node_modules/jest/bin/jest.js",
    "test:watch": "node --experimental-vm-modules ./node_modules/jest/bin/jest.js --watch",
    "test:coverage": "node --experimental-vm-modules ./node_modules/jest/bin/jest.js --coverage",
    "test:load": "k6 run tests/load/graphql-test.js",
    "test:load:smoke": "k6 run --vus 1 --duration 10s tests/load/graphql-test.js",
    "test:load:stress": "k6 run -e K6_SCENARIO=stress tests/load/graphql-test.js",
    "build": "tsc",
    "dev": "tsx watch src/index.ts",
    "build1": "tsc",
    "start": "node dist/agentcore-evaluations-example.js",
    "dev1": "ts-node agentcore-evaluations-example.ts",
    "dev:real": "ts-node agentcore-evaluations-real.ts",
    "dev:promptfoo": "ts-node promptfoo-evaluations.ts",
    "eval1": "npx promptfoo eval",
    "eval:no-cache": "npx promptfoo eval --no-cache",
    "eval:advanced": "npx promptfoo eval --config promptfooconfig-advanced.yaml",
    "eval:modular": "npx promptfoo eval --config promptfooconfig-modular.yaml",
    "eval:all-features": "npx promptfoo eval --config promptfooconfig-all-features.yaml",
    "eval:scenarios": "npx promptfoo eval --config promptfooconfig-scenarios.yaml",
    "eval:comparison": "npx promptfoo eval --config promptfooconfig-comparison.yaml",
    "eval:python": "npx promptfoo eval --config promptfooconfig-python-assertions.yaml",
    "eval:astro": "npx promptfoo eval --config promptfooconfig-astro-graphql.yaml",
    "eval:astro-extended": "npx promptfoo eval --config promptfooconfig-astro-graphql-extended.yaml",
    "eval:astro-tracing": "npx promptfoo eval --config promptfooconfig-astro-graphql-with-tracing.yaml",
    "eval:astro-jaeger": "npx promptfoo eval --config promptfooconfig-astro-graphql-jaeger.yaml",
    "eval:astro-dual": "npx promptfoo eval --config promptfooconfig-astro-graphql-dual-export.yaml",
    "tracing:start": "docker-compose --profile tracing up -d",
    "tracing:stop": "docker-compose --profile tracing down",
    "tracing:jaeger": "start http://localhost:16686",
    "xray:start": "docker-compose --profile xray up -d",
    "xray:stop": "docker-compose --profile xray down",
    "tempo:start": "docker-compose --profile tempo up -d",
    "tempo:stop": "docker-compose --profile tempo down",
    "grafana:open": "start http://localhost:3000",
    "eval:view1": "npx promptfoo view --port 3210",
    "eval:staging": "dotenv -e .env.staging -- npx promptfoo eval",
    "eval:production": "dotenv -e .env.production -- npx promptfoo eval --no-cache",
    "cache:clear": "npx promptfoo cache clear",
    "generate": "npx promptfoo generate dataset --config promptfooconfig.yaml --output datasets/generated-tests.yaml",
    "generate:de": "npx promptfoo generate dataset --config promptfooconfig.yaml --instructions \"Generiere Test-Fragen auf Deutsch fÃ¼r einen Fahrzeuginformations-Assistenten. Themen: VIN, LKW-Technik, Elektro-LKW, Abgasnormen.\" --numPersonas 5 --numTestCasesPerPersona 4 --output datasets/generated-tests.yaml",
    "redteam": "npx promptfoo redteam run --config redteamconfig.yaml",
    "redteam:report": "npx promptfoo redteam report",
    "redteam:setup": "npx promptfoo redteam setup",
    "redteam:full": "npm run redteam && npm run redteam:report && npx promptfoo view",
    "test1": "vitest run",
    "test:watch1": "vitest",
    "test:coverage1": "vitest run --coverage"
  },
  "dependencies": {
    "@aws-sdk/client-bedrock-agentcore": "^3.978.0",
    "@aws-sdk/client-bedrock-agentcore-control": "^3.978.0",
    "@aws-sdk/client-bedrock-runtime": "^3.700.0",
    "@aws-sdk/client-cloudwatch-logs": "^3.978.0",
    "@opentelemetry/api": "^1.9.0",
    "@opentelemetry/exporter-trace-otlp-http": "^0.57.0",
    "@opentelemetry/id-generator-aws-xray": "^1.2.3",
    "@opentelemetry/propagator-aws-xray": "^1.26.2",
    "@opentelemetry/resources": "^1.30.0",
    "@opentelemetry/sdk-trace-base": "^1.30.0",
    "@opentelemetry/sdk-trace-node": "^1.30.0",
    "@opentelemetry/semantic-conventions": "^1.28.0",
    "axios": "^1.13.4",
    "langfuse": "^3.0.0",
    "pino": "^10.3.0",
    "pino-pretty": "^13.1.3",
    "promptfoo": "^0.120.20"
  },
  "devDependencies": {
    "@types/jest": "^30.0.0",
    "@types/node": "^22.0.0",
    "@types/pino": "^7.0.4",
    "dotenv-cli": "^11.0.0",
    "fast-check": "^4.5.3",
    "jest": "^30.2.0",
    "promptfoo": "^0.120.20",
    "ts-jest": "^29.4.6",
    "ts-node": "^10.9.2",
    "tsx": "^4.16.0",
    "typescript": "^5.3.0",
    "vitest": "^2.0.0"
  },
  "engines": {
    "node": ">=18.0.0"
  },
  "keywords": [
    "llm",
    "evaluation",
    "observability",
    "opentelemetry",
    "tracing",
    "promptfoo",
    "aws",
    "bedrock",
    "xray",
    "jaeger",
    "genai",
    "testing"
  ],
  "license": "MIT"
}