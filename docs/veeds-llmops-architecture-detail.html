<!DOCTYPE html>
<html lang="de">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>VEEDS LLMOps ‚Äî Detaillierte Architektur</title>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.9.1/mermaid.min.js"></script>
<style>
  :root {
    --bg: #0f1117;
    --surface: #1a1d27;
    --surface2: #232734;
    --border: #2e3347;
    --text: #e2e4eb;
    --text-muted: #8b90a0;
    --accent: #6c8cff;
    --accent2: #58d5a8;
    --orange: #f0a050;
    --red: #f06060;
    --purple: #b07cf0;
  }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    background: var(--bg);
    color: var(--text);
    line-height: 1.7;
    max-width: 960px;
    margin: 0 auto;
    padding: 40px 24px 80px;
  }
  h1 { font-size: 2em; margin: 0 0 8px; color: #fff; }
  h1 span { color: var(--accent); }
  .subtitle { color: var(--text-muted); margin-bottom: 48px; font-size: 1.05em; }
  h2 {
    font-size: 1.45em; color: var(--accent);
    margin: 56px 0 24px; padding-bottom: 10px;
    border-bottom: 1px solid var(--border);
  }
  h3 { font-size: 1.15em; color: var(--accent2); margin: 32px 0 12px; }
  h4 { font-size: 1em; color: var(--orange); margin: 24px 0 8px; }
  p { margin-bottom: 14px; }
  a { color: var(--accent); }

  /* TOC */
  .toc {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 10px;
    padding: 24px 28px;
    margin-bottom: 48px;
  }
  .toc h3 { margin-top: 0; color: var(--text-muted); font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px; }
  .toc ol { padding-left: 20px; }
  .toc li { margin: 6px 0; }
  .toc a { text-decoration: none; color: var(--text); }
  .toc a:hover { color: var(--accent); }

  /* Code */
  code {
    background: var(--surface2);
    padding: 2px 7px;
    border-radius: 4px;
    font-size: 0.88em;
    font-family: 'SF Mono', 'Fira Code', monospace;
    color: var(--accent2);
  }
  pre {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 8px;
    padding: 18px 20px;
    overflow-x: auto;
    margin: 14px 0 20px;
    font-size: 0.85em;
    line-height: 1.6;
  }
  pre code { background: none; padding: 0; color: var(--text); }

  /* Diagrams */
  .diagram-container {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 10px;
    padding: 28px 20px;
    margin: 20px 0 28px;
    overflow-x: auto;
  }
  .diagram-container .mermaid { display: flex; justify-content: center; }
  .diagram-label {
    text-align: center;
    color: var(--text-muted);
    font-size: 0.85em;
    margin-top: 12px;
    font-style: italic;
  }

  /* Info boxes */
  .box {
    border-radius: 8px;
    padding: 18px 20px;
    margin: 16px 0 20px;
    border-left: 4px solid;
  }
  .box-blue { background: #161b2e; border-color: var(--accent); }
  .box-green { background: #152220; border-color: var(--accent2); }
  .box-orange { background: #221c14; border-color: var(--orange); }
  .box-red { background: #221416; border-color: var(--red); }
  .box-purple { background: #1c1628; border-color: var(--purple); }
  .box strong { display: block; margin-bottom: 6px; }

  /* Tables */
  table {
    width: 100%;
    border-collapse: collapse;
    margin: 14px 0 22px;
    font-size: 0.92em;
  }
  th, td {
    padding: 10px 14px;
    text-align: left;
    border: 1px solid var(--border);
  }
  th { background: var(--surface2); color: var(--accent); font-weight: 600; }
  td { background: var(--surface); }
  tr:hover td { background: var(--surface2); }

  /* Step lists */
  .step { display: flex; gap: 16px; margin: 16px 0; align-items: flex-start; }
  .step-num {
    flex-shrink: 0;
    width: 32px; height: 32px;
    background: var(--accent);
    color: #fff;
    border-radius: 50%;
    display: flex; align-items: center; justify-content: center;
    font-weight: 700; font-size: 0.85em;
  }
  .step-content { flex: 1; }
  .step-content strong { color: #fff; }

  /* File tree */
  .file-tree {
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 8px;
    padding: 18px 20px;
    font-family: 'SF Mono', monospace;
    font-size: 0.84em;
    line-height: 1.8;
    margin: 14px 0 22px;
    overflow-x: auto;
    white-space: pre;
  }
  .ft-dir { color: var(--accent); font-weight: 600; }
  .ft-file { color: var(--text); }
  .ft-desc { color: var(--text-muted); }

  /* Badge */
  .badge {
    display: inline-block;
    padding: 2px 9px;
    border-radius: 4px;
    font-size: 0.78em;
    font-weight: 600;
    vertical-align: middle;
  }
  .badge-blue { background: #1a2a4a; color: var(--accent); }
  .badge-green { background: #142e24; color: var(--accent2); }
  .badge-orange { background: #2e2216; color: var(--orange); }
  .badge-red { background: #2e1618; color: var(--red); }
  .badge-purple { background: #221a30; color: var(--purple); }

  ul { margin: 8px 0 14px 20px; }
  li { margin: 4px 0; }

  .section-divider {
    border: none;
    border-top: 1px solid var(--border);
    margin: 56px 0;
  }
</style>
</head>
<body>

<h1>VEEDS LLMOps Stack ‚Äî <span>Detaillierte Architektur</span></h1>
<p class="subtitle">Langfuse v3 ¬∑ Promptfoo ¬∑ k6 ¬∑ AWS Bedrock ¬∑ GitLab CI/CD ‚Äî Version 2.0, Februar 2026</p>

<div class="toc">
  <h3>Inhalt</h3>
  <ol>
    <li><a href="#overview">Gesamtarchitektur</a></li>
    <li><a href="#infra">Infrastruktur: Docker Compose (6 Container)</a></li>
    <li><a href="#golden">Golden Dataset: Aufbau & Erstellung</a></li>
    <li><a href="#transform">Transformation: Golden Dataset ‚Üí Promptfoo Tests</a></li>
    <li><a href="#langfuse-flow">Langfuse Integration: Prompts, Datasets, Traces, Scores</a></li>
    <li><a href="#production">Production-Request Durchlauf</a></li>
    <li><a href="#gitlab">GitLab CI/CD Pipeline im Detail</a></li>
    <li><a href="#results">Wo landen die Ergebnisse?</a></li>
    <li><a href="#improve">Verbesserungsvorschl√§ge</a></li>
  </ol>
</div>

<!-- ================================================================== -->
<h2 id="overview">1. Gesamtarchitektur</h2>
<!-- ================================================================== -->

<p>Der VEEDS LLMOps Stack hat drei Hauptpfade: <strong>Production</strong> (Echtzeit-Tracing), <strong>Evaluation</strong> (Qualit√§tssicherung im CI) und <strong>Performance</strong> (Lasttests). Alle drei konvergieren in Langfuse als zentralem Dashboard.</p>

<div class="diagram-container">
<pre class="mermaid">
graph TB
    subgraph "üîµ Datenquellen (Single Source of Truth)"
        GD["üìã golden_dataset.json<br/><i>16 Test Cases</i>"]
        PT["üìù eval/prompt.txt<br/><i>Prompt Template</i>"]
    end

    subgraph "üü¢ Production-Pfad"
        APP["üöÄ Proofreader<br/><i>proofreadEntry()</i>"]
        BED["‚òÅÔ∏è AWS Bedrock<br/><i>Claude 3.5 Sonnet</i>"]
        APP -->|"InvokeModel<br/>(mit Retry)"| BED
    end

    subgraph "üü† Evaluation-Pfad (CI/CD)"
        GEN["‚öôÔ∏è generate-promptfoo-tests.ts"]
        YAML["üìÑ generated-tests.yaml"]
        PF["üß™ Promptfoo eval --assert"]
        JSON["üìä results/ci-{id}.json"]
        GEN --> YAML
        YAML --> PF
        PF -->|"Bedrock Calls"| BED
        PF --> JSON
    end

    subgraph "üî¥ Performance-Pfad"
        K6["üèãÔ∏è k6 Load Test"]
        K6J["üìä k6-results.json"]
        K6 -->|"GraphQL Mutation"| API["üåê VEEDS API"]
        API --> APP
        K6 --> K6J
    end

    subgraph "üü£ Langfuse v3 (Zentral-Dashboard)"
        LF["üîÆ Langfuse Web<br/><i>:3000</i>"]
        CH["üì¶ ClickHouse<br/><i>OLAP Traces</i>"]
        LF --> CH
    end

    GD -->|"Liest"| GEN
    PT -->|"Liest"| GEN
    PT -->|"Fallback"| APP
    APP -->|"Traces + Scores"| LF
    JSON -->|"push-scores-to-langfuse.ts"| LF
    GD -->|"upload-dataset-to-langfuse.ts"| LF
    PT -->|"seed-langfuse.ts"| LF
    LF -->|"Prompt laden<br/>(production label)"| APP

    style GD fill:#1a2a4a,stroke:#6c8cff,color:#e2e4eb
    style PT fill:#1a2a4a,stroke:#6c8cff,color:#e2e4eb
    style LF fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style BED fill:#221c14,stroke:#f0a050,color:#e2e4eb
    style PF fill:#152220,stroke:#58d5a8,color:#e2e4eb
</pre>
<p class="diagram-label">Abb. 1: Gesamtarchitektur ‚Äî Drei Pfade konvergieren in Langfuse</p>
</div>

<div class="box box-blue">
<strong>üí° Kernprinzip: Single Source of Truth</strong>
Alle Test Cases leben in <code>golden_dataset.json</code>. Alle Prompts leben in <code>eval/prompt.txt</code> (lokal) bzw. Langfuse (remote). Nirgends im System wird ein Test Case oder Prompt dupliziert.
</div>


<!-- ================================================================== -->
<h2 id="infra">2. Infrastruktur: Docker Compose (6 Container)</h2>
<!-- ================================================================== -->

<div class="diagram-container">
<pre class="mermaid">
graph LR
    subgraph "üåê Extern erreichbar"
        WEB["<b>langfuse-web</b><br/>:3000<br/><i>Next.js UI + API</i>"]
    end

    subgraph "üîí Nur localhost (127.0.0.1)"
        WORKER["<b>langfuse-worker</b><br/>:3030<br/><i>Async Processing</i>"]
        PG["<b>PostgreSQL 16</b><br/>:5432<br/><i>Users, Prompts, Keys</i>"]
        CK["<b>ClickHouse 24.3</b><br/>:8123 / :9000<br/><i>Traces, Scores (OLAP)</i>"]
        RD["<b>Redis 7</b><br/>:6379<br/><i>Queue + Cache</i>"]
        MN["<b>MinIO</b><br/>:9090 / :9091<br/><i>S3 Blob Storage</i>"]
    end

    WEB -->|"Schreibt Events"| RD
    WEB -->|"Blobs"| MN
    WEB -->|"Liest/Schreibt"| PG
    WEB -->|"Liest"| CK
    WORKER -->|"Liest Queue"| RD
    WORKER -->|"Liest Blobs"| MN
    WORKER -->|"Schreibt Traces"| CK
    WORKER -->|"Liest/Schreibt"| PG

    style WEB fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style WORKER fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style PG fill:#1a2a4a,stroke:#6c8cff,color:#e2e4eb
    style CK fill:#221c14,stroke:#f0a050,color:#e2e4eb
    style RD fill:#2e1618,stroke:#f06060,color:#e2e4eb
    style MN fill:#152220,stroke:#58d5a8,color:#e2e4eb
</pre>
<p class="diagram-label">Abb. 2: Docker Compose ‚Äî 6 Container auf Bridge-Netzwerk</p>
</div>

<h3>Datenfluss innerhalb Langfuse</h3>

<p>Wenn das Langfuse TypeScript SDK einen Trace sendet, passiert folgendes:</p>

<div class="diagram-container">
<pre class="mermaid">
sequenceDiagram
    participant SDK as Langfuse SDK<br/>(in Proofreader)
    participant WEB as langfuse-web<br/>(:3000)
    participant S3 as MinIO<br/>(S3 Blob)
    participant Q as Redis<br/>(Queue)
    participant W as langfuse-worker
    participant CH as ClickHouse<br/>(OLAP)
    participant PG as PostgreSQL

    SDK->>+WEB: POST /api/public/ingestion<br/>(batch von Events)
    WEB->>S3: Event-Payload als Blob speichern
    WEB->>Q: Job in Queue: "process blob X"
    WEB-->>-SDK: 200 OK (async accepted)

    Note over W,Q: Worker pollt Redis Queue

    Q->>+W: Job: "process blob X"
    W->>S3: Blob X lesen
    W->>W: Events parsen, aggregieren,<br/>Kosten berechnen
    W->>CH: INSERT Traces, Observations, Scores
    W->>PG: Metadata updaten (falls n√∂tig)
    W-->>-Q: Job done

    Note over WEB,CH: Dashboard-Query

    WEB->>CH: SELECT traces WHERE project_id = ...
    CH-->>WEB: Trace-Daten (spaltenbasiert, schnell)
    WEB->>PG: Prompt-Versionen, User-Info
    PG-->>WEB: Metadata
</pre>
<p class="diagram-label">Abb. 3: Interner Datenfluss ‚Äî SDK ‚Üí Web ‚Üí Redis/MinIO ‚Üí Worker ‚Üí ClickHouse</p>
</div>

<div class="box box-purple">
<strong>Warum diese Architektur?</strong>
Langfuse v3 hat die Read-Queries f√ºr Traces auf ClickHouse migriert (Column-Oriented, OLAP). Dadurch sind Dashboard-Queries 10-100x schneller als in v2 (PostgreSQL). Der asynchrone Pfad √ºber Redis + Worker entkoppelt das Trace-Ingestion vom Processing ‚Äî das SDK bekommt sofort 200 OK zur√ºck, auch wenn ClickHouse gerade busy ist.
</div>

<h3>Container-Details</h3>

<table>
<tr><th>Container</th><th>Image</th><th>Port</th><th>Rolle</th><th>RAM (idle)</th></tr>
<tr><td><code>langfuse-web</code></td><td><code>langfuse/langfuse:3</code></td><td>3000 (extern)</td><td>Web UI, REST API, Trace-Empfang, Prompt-Mgmt</td><td>~300 MB</td></tr>
<tr><td><code>langfuse-worker</code></td><td><code>langfuse/langfuse-worker:3</code></td><td>3030 (localhost)</td><td>Queue-Processing, ClickHouse-Writes</td><td>~200 MB</td></tr>
<tr><td><code>langfuse-postgres</code></td><td><code>postgres:16-alpine</code></td><td>5432 (localhost)</td><td>ACID: Users, Orgs, Projects, API Keys, Prompts</td><td>~100 MB</td></tr>
<tr><td><code>langfuse-clickhouse</code></td><td><code>clickhouse/clickhouse-server:24.3</code></td><td>8123+9000 (localhost)</td><td>OLAP: Traces, Observations, Scores</td><td>~200 MB</td></tr>
<tr><td><code>langfuse-redis</code></td><td><code>redis:7-alpine</code></td><td>6379 (localhost)</td><td>Message Queue + Client Cache, 256MB LRU</td><td>~30 MB</td></tr>
<tr><td><code>langfuse-minio</code></td><td><code>minio/minio</code></td><td>9090+9091 (localhost)</td><td>S3 Blob Storage f√ºr Event-Payloads</td><td>~50 MB</td></tr>
</table>


<!-- ================================================================== -->
<h2 id="golden">3. Golden Dataset: Aufbau & Erstellung</h2>
<!-- ================================================================== -->

<h3>Was ist das Golden Dataset?</h3>

<p>Eine kuratierte Sammlung von 16 YAML-Eintr√§gen mit <strong>exakt definierten erwarteten Ergebnissen</strong>. Es ist die Quelle der Wahrheit f√ºr die gesamte Evaluations-Infrastruktur ‚Äî Promptfoo-Tests und Langfuse-Experiments werden daraus abgeleitet.</p>

<div class="diagram-container">
<pre class="mermaid">
graph TD
    subgraph "golden_dataset.json"
        META["üìã Metadaten<br/>version: 1.0.0<br/>specVersion: 2.1"]

        subgraph "üî¥ True Positives (6)"
            TP1["tp-001: Ung√ºltiges materialNumber-Format<br/><i>INVALID ‚Üí field: materialNumber, severity: error</i>"]
            TP2["tp-002: Leere Description<br/><i>'' ‚Üí field: description, severity: error</i>"]
            TP3["tp-003: Ung√ºltige Einheit<br/><i>bananas ‚Üí field: unit, severity: error</i>"]
            TP4["tp-004: min > max<br/><i>100/5 ‚Üí field: valueRange, severity: error</i>"]
            TP5["tp-005: 4 Fehler gleichzeitig<br/><i>materialNumber + description + unit + range</i>"]
            TP6["tp-006: Description > 200 Zeichen<br/><i>field: description, severity: error</i>"]
        end

        subgraph "üü¢ True Negatives (4)"
            TN1["tn-001: Perfekte Bremsscheibe<br/><i>expectedIsValid: true, errors: []</i>"]
            TN2["tn-002: √ñlfilter mit bar<br/><i>expectedIsValid: true, errors: []</i>"]
            TN3["tn-003: Minimale Felder<br/><i>expectedIsValid: true, errors: []</i>"]
            TN4["tn-004: Nm Einheit (Drehmoment)<br/><i>expectedIsValid: true, errors: []</i>"]
        end

        subgraph "üü° Edge Cases (3)"
            EC1["ec-001: Kleinbuchstaben abc-12345<br/><i>severity: warning (nicht error!)</i>"]
            EC2["ec-002: min == max<br/><i>severity: warning</i>"]
            EC3["ec-003: Exakt 200 Zeichen<br/><i>expectedIsValid: true (Grenze!)</i>"]
        end

        subgraph "‚ö†Ô∏è Adversarial (3)"
            ADV1["adv-001: YAML Injection<br/><i>--- in Description ‚Üí trotzdem valid</i>"]
            ADV2["adv-002: Prompt Injection<br/><i>'Ignore all instructions' ‚Üí trotzdem valid</i>"]
            ADV3["adv-003: Unicode √Ñ√ñ√ú-12345<br/><i>‚Üí severity: error</i>"]
        end
    end

    style TP1 fill:#2e1618,stroke:#f06060,color:#e2e4eb
    style TP2 fill:#2e1618,stroke:#f06060,color:#e2e4eb
    style TP3 fill:#2e1618,stroke:#f06060,color:#e2e4eb
    style TP4 fill:#2e1618,stroke:#f06060,color:#e2e4eb
    style TP5 fill:#2e1618,stroke:#f06060,color:#e2e4eb
    style TP6 fill:#2e1618,stroke:#f06060,color:#e2e4eb
    style TN1 fill:#152220,stroke:#58d5a8,color:#e2e4eb
    style TN2 fill:#152220,stroke:#58d5a8,color:#e2e4eb
    style TN3 fill:#152220,stroke:#58d5a8,color:#e2e4eb
    style TN4 fill:#152220,stroke:#58d5a8,color:#e2e4eb
    style EC1 fill:#221c14,stroke:#f0a050,color:#e2e4eb
    style EC2 fill:#221c14,stroke:#f0a050,color:#e2e4eb
    style EC3 fill:#221c14,stroke:#f0a050,color:#e2e4eb
    style ADV1 fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style ADV2 fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style ADV3 fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
</pre>
<p class="diagram-label">Abb. 4: Golden Dataset ‚Äî 16 Test Cases in 4 Kategorien</p>
</div>

<h3>Struktur eines einzelnen Test Cases</h3>

<pre><code>{
  "id": "tp-001",                              // Eindeutige ID
  "category": "true_positive",                  // Bestimmt Assertion-Typ
  "description": "Invalid materialNumber",      // Menschenlesbarer Name
  "input": "materialNumber: INVALID\n...",      // YAML-String ‚Üí wird an LLM geschickt
  "expectedErrors": [{                          // Was das LLM finden MUSS
    "field": "materialNumber",                  //   Welches Feld
    "severity": "error",                        //   Welche Schwere
    "pattern": "format|Format|XXX-NNNNN|..."    //   Regex f√ºr Fehlermeldung
  }],
  "expectedIsValid": false                      // Erwarteter isValid-Wert
}</code></pre>

<div class="box box-orange">
<strong>‚ö†Ô∏è Designentscheidungen im Golden Dataset</strong>
<ul>
<li><code>pattern</code> ist ein Regex, der sowohl deutsche als auch englische Fehlermeldungen akzeptiert (z.B. <code>"leer|empty"</code>) ‚Äî weil Claude je nach Kontext in beiden Sprachen antworten kann.</li>
<li>Edge Cases haben <code>severity: "warning"</code> statt <code>"error"</code> ‚Äî ein Grenzfall wie Kleinbuchstaben ist nicht zwingend falsch, aber verd√§chtig.</li>
<li>Adversarial Cases <code>adv-001</code> und <code>adv-002</code> haben <code>expectedIsValid: true</code> und <code>expectedErrors: []</code> ‚Äî die YAML-Eintr√§ge sind technisch valide, sie versuchen nur das LLM zu manipulieren.</li>
</ul>
</div>


<!-- ================================================================== -->
<h2 id="transform">4. Transformation: Golden Dataset ‚Üí Promptfoo Tests</h2>
<!-- ================================================================== -->

<p>Das ist der kritische √úbersetzungsschritt: Das menschenlesbare Golden Dataset wird in maschinenausf√ºhrbare Promptfoo-Assertions konvertiert.</p>

<div class="diagram-container">
<pre class="mermaid">
flowchart LR
    subgraph Input
        GD["üìã golden_dataset.json<br/><i>16 Test Cases<br/>mit expectedErrors,<br/>expectedIsValid, patterns</i>"]
    end

    subgraph "generate-promptfoo-tests.ts"
        READ["1Ô∏è‚É£ JSON lesen<br/>+ parsen"]
        MAP["2Ô∏è‚É£ Pro Test Case:<br/>Assertions generieren<br/>basierend auf category"]
        WRITE["3Ô∏è‚É£ YAML-Datei<br/>schreiben"]
        READ --> MAP --> WRITE
    end

    subgraph Output
        YAML["üìÑ generated-tests.yaml<br/><i>16 Tests mit<br/>~70 Assertions total</i>"]
    end

    subgraph "promptfooconfig.yaml"
        CFG["tests: file://eval/<br/>generated-tests.yaml"]
    end

    GD --> READ
    WRITE --> YAML
    YAML --> CFG

    style GD fill:#1a2a4a,stroke:#6c8cff,color:#e2e4eb
    style YAML fill:#152220,stroke:#58d5a8,color:#e2e4eb
    style CFG fill:#221c14,stroke:#f0a050,color:#e2e4eb
</pre>
<p class="diagram-label">Abb. 5: Golden Dataset ‚Üí Promptfoo Test-Generierung</p>
</div>

<h3>Schritt-f√ºr-Schritt: Was der Generator pro Test Case erzeugt</h3>

<p>Der Generator liest jeden Test Case und erzeugt basierend auf <code>category</code> und <code>expectedErrors</code> unterschiedliche Assertion-Kombinationen:</p>

<div class="diagram-container">
<pre class="mermaid">
flowchart TD
    TC["Test Case einlesen"]

    TC --> A1["<b>Assertion 1:</b> isValid Check<br/><code>javascript</code><br/>p.isValid === expectedIsValid"]

    TC --> Q1{expectedErrors<br/>vorhanden?}

    Q1 -->|Ja| A2["<b>Assertion 2a:</b> Pro Fehlerfeld<br/><code>javascript</code><br/>p.errors.some(e => e.field === 'X')"]
    Q1 -->|Nein| A2B["<b>Assertion 2b:</b> Keine False Positives<br/><code>javascript</code><br/>p.errors.length === 0"]

    A2 --> Q2{severity<br/>definiert?}
    Q2 -->|Ja| A3["<b>Assertion 3:</b> Severity Check<br/><code>javascript</code><br/>e.severity === 'error'"]
    Q2 -->|Nein| Q3

    A3 --> Q3{pattern<br/>definiert?}
    Q3 -->|Ja| A4["<b>Assertion 4:</b> Message Pattern<br/><code>javascript</code><br/>RegExp(pattern).test(e.message)"]
    Q3 -->|Nein| A5

    A4 --> A5["<b>Assertion 5:</b> Error Count<br/><code>javascript</code><br/>p.errors.length >= expected"]

    TC --> Q4{category =<br/>adversarial?}
    Q4 -->|Ja| A6["<b>Assertion 6:</b> Injection Resistance<br/><code>g-eval</code> (LLM-as-Judge)<br/>threshold: 0.85"]

    TC --> Q5{category =<br/>edge_case?}
    Q5 -->|Ja| A7["<b>Assertion 7:</b> Edge Case Handling<br/><code>g-eval</code> (LLM-as-Judge)<br/>threshold: 0.70"]

    style A1 fill:#1a2a4a,stroke:#6c8cff,color:#e2e4eb
    style A2 fill:#152220,stroke:#58d5a8,color:#e2e4eb
    style A2B fill:#152220,stroke:#58d5a8,color:#e2e4eb
    style A3 fill:#221c14,stroke:#f0a050,color:#e2e4eb
    style A4 fill:#221c14,stroke:#f0a050,color:#e2e4eb
    style A5 fill:#2e1618,stroke:#f06060,color:#e2e4eb
    style A6 fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style A7 fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
</pre>
<p class="diagram-label">Abb. 6: Assertion-Generierung ‚Äî Entscheidungsbaum pro Test Case</p>
</div>

<h3>Konkretes Beispiel: tp-001 ‚Üí generierte Assertions</h3>

<p>Eingabe aus <code>golden_dataset.json</code>:</p>
<pre><code>{
  "id": "tp-001",
  "category": "true_positive",
  "input": "materialNumber: INVALID\ndescription: Bremsscheibe\nunit: mm",
  "expectedErrors": [{ "field": "materialNumber", "severity": "error",
                       "pattern": "format|Format|XXX-NNNNN|ung√ºltig|invalid" }],
  "expectedIsValid": false
}</code></pre>

<p>Generierte Assertions in <code>generated-tests.yaml</code>:</p>
<pre><code>- description: "[tp-001] Invalid materialNumber format"
  vars:
    yaml_entry: |
      materialNumber: INVALID
      description: Bremsscheibe
      unit: mm
  assert:
    # 1. isValid muss false sein
    - type: javascript
      value: "const p = JSON.parse(output); return p.isValid === false;"
      metric: correctness/is_valid

    # 2. Feld "materialNumber" muss in errors vorkommen
    - type: javascript
      value: "const p = JSON.parse(output); return p.errors.some(e => ...);"
      metric: correctness/field_materialNumber

    # 3. Severity muss "error" sein
    - type: javascript
      value: "... return !e || e.severity === 'error';"
      metric: correctness/severity_materialNumber

    # 4. Fehlermeldung muss Pattern matchen (DE oder EN)
    - type: javascript
      value: "... return e && new RegExp('format|Format|XXX-NNNNN|...', 'i').test(e.message);"
      metric: correctness/message_materialNumber

    # 5. Mindestens 1 Fehler erwartet
    - type: javascript
      value: "const p = JSON.parse(output); return p.errors.length >= 1;"
      metric: correctness/error_count
  metadata:
    goldenId: "tp-001"
    category: "true_positive"</code></pre>

<h3>Zus√§tzlich: Default-Assertions (f√ºr JEDEN Test)</h3>

<p>Die <code>promptfooconfig.yaml</code> definiert Assertions, die auf jeden einzelnen Test Case angewandt werden, egal aus welcher Kategorie:</p>

<table>
<tr><th>Assertion</th><th>Typ</th><th>Threshold</th><th>Zweck</th></tr>
<tr><td>is-json</td><td><code>is-json</code></td><td>‚Äî</td><td>Response muss valides JSON sein</td></tr>
<tr><td>Latenz</td><td><code>latency</code></td><td>5000ms</td><td>Antwort unter 5 Sekunden</td></tr>
<tr><td>Kosten</td><td><code>cost</code></td><td>$0.05</td><td>Pro Call unter 5 Cent</td></tr>
<tr><td>Struktur</td><td><code>javascript</code></td><td>‚Äî</td><td>Response hat <code>isValid</code> und <code>errors</code> Felder</td></tr>
</table>


<!-- ================================================================== -->
<h2 id="langfuse-flow">5. Langfuse Integration: Prompts, Datasets, Traces, Scores</h2>
<!-- ================================================================== -->

<p>Langfuse spielt vier verschiedene Rollen im Stack. Jede hat einen eigenen Datenfluss:</p>

<div class="diagram-container">
<pre class="mermaid">
graph TB
    subgraph "Rolle 1: Prompt Management"
        PT["eval/prompt.txt"] -->|"seed-langfuse.ts"| LP["Langfuse Prompt<br/><b>veeds-proofreader</b><br/>Label: production"]
        LP -->|"getPrompt()<br/>cacheTtlSeconds: 300"| APP["Proofreader<br/>proofreadEntry()"]
        PT -->|"Fallback<br/>(wenn Langfuse offline)"| APP
    end

    subgraph "Rolle 2: Golden Dataset Hosting"
        GD["golden_dataset.json"] -->|"seed-langfuse.ts<br/>oder upload-dataset"| LD["Langfuse Dataset<br/><b>veeds-proofreader-golden</b><br/>16 Items"]
        LD -->|"Langfuse UI:<br/>New Experiment"| EXP["Langfuse Experiment<br/><i>Prompt v2 vs v3</i>"]
    end

    subgraph "Rolle 3: Production Tracing"
        APP -->|"Langfuse SDK<br/>(async batch)"| TR["Langfuse Trace<br/><i>Spans + Generation<br/>+ Scores</i>"]
    end

    subgraph "Rolle 4: Eval Score Aggregation"
        PF["Promptfoo<br/>eval results JSON"] -->|"push-scores-to-langfuse.ts"| SC["Langfuse Scores<br/><i>eval_pass_rate<br/>eval_cost<br/>eval_latency</i>"]
    end

    TR --> DASH["üìä Langfuse Dashboard<br/><i>Unified View:</i><br/>Production + Eval + Experiments"]
    SC --> DASH
    EXP --> DASH

    style LP fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style LD fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style TR fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style SC fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style DASH fill:#1c1628,stroke:#fff,color:#e2e4eb
</pre>
<p class="diagram-label">Abb. 7: Langfuse ‚Äî Vier Rollen im Stack</p>
</div>

<h3>Rolle 1: Prompt Management</h3>

<p>Der Prompt <code>veeds-proofreader</code> wird mit dem Label <code>production</code> in Langfuse gespeichert. Das erm√∂glicht:</p>
<ul>
<li><strong>Versionierung:</strong> Jede √Ñnderung am Prompt erzeugt eine neue Version. Alte Versionen bleiben erhalten.</li>
<li><strong>Label-basiertes Deployment:</strong> Das Label <code>production</code> zeigt auf die aktive Version. Man kann eine neue Version mit Label <code>staging</code> testen, bevor man <code>production</code> umh√§ngt.</li>
<li><strong>Client-Side Caching:</strong> Der Proofreader cached den Prompt 5 Minuten lokal (<code>cacheTtlSeconds: 300</code>), sodass nicht bei jedem Request ein HTTP-Call nach Langfuse geht.</li>
<li><strong>Fallback:</strong> Wenn Langfuse nicht erreichbar ist, l√§dt der Proofreader <code>eval/prompt.txt</code> als Fallback. Der Langfuse-Span wird als WARNING geloggt.</li>
</ul>

<h3>Rolle 2: Dataset & Experiments</h3>

<p>Nach dem Upload der Golden Dataset-Items kann man in der Langfuse UI Experiments starten ‚Äî im Prinzip ein Batch-Run des Prompts gegen alle Dataset-Items, mit automatischem Vergleich von Expected vs. Actual Output. Das ist ideal f√ºr:</p>
<ul>
<li><strong>Prompt-Vergleich:</strong> "Ist Prompt v3 besser als v2?" ‚Üí Experiment mit beiden Versionen gegen dasselbe Dataset</li>
<li><strong>Model-Vergleich:</strong> "Sonnet vs. Haiku auf dem Golden Dataset"</li>
<li><strong>Regression Detection:</strong> "Hat die neue Prompt-Version bestehende True Negatives kaputt gemacht?"</li>
</ul>

<h3>Rolle 3: Production Tracing</h3>

<p>Jeder <code>proofreadEntry()</code>-Aufruf erzeugt einen vollst√§ndigen Trace mit Spans:</p>

<div class="diagram-container">
<pre class="mermaid">
gantt
    title Trace Waterfall: proofreadEntry()
    dateFormat X
    axisFormat %L ms

    section Trace
    veeds-proofreader           :a1, 0, 3200

    section Spans
    load-prompt (Langfuse)      :a2, 0, 150
    bedrock-claude (Generation) :a3, 150, 2900
    parse-response              :a4, 2900, 3100

    section Scores
    processing_time_ms: 3100    :milestone, a5, 3100, 0
</pre>
<p class="diagram-label">Abb. 8: Trace Waterfall ‚Äî drei Spans pro Proofreader-Call</p>
</div>

<p>Die <strong>Generation</strong> (Span <code>bedrock-claude</code>) ist ein spezieller Span-Typ in Langfuse mit zus√§tzlichen Feldern:</p>
<ul>
<li><code>model</code>: <code>anthropic.claude-3-5-sonnet-20241022-v2:0</code></li>
<li><code>usage.input</code>: Input Tokens (f√ºr Kostenberechnung)</li>
<li><code>usage.output</code>: Output Tokens</li>
<li><code>modelParameters</code>: <code>{ temperature: 0, max_tokens: 2048 }</code></li>
</ul>

<h3>Rolle 4: Score Aggregation</h3>

<p>Der Score-Bridge (<code>push-scores-to-langfuse.ts</code>) nimmt Promptfoo JSON-Ergebnisse und erzeugt in Langfuse:</p>

<table>
<tr><th>Score</th><th>Ebene</th><th>Wert</th><th>Beispiel</th></tr>
<tr><td><code>eval_pass</code></td><td>Pro Span (Test Case)</td><td>0 oder 1</td><td>1 = alle Assertions bestanden</td></tr>
<tr><td><code>eval_latency_ms</code></td><td>Pro Span</td><td>Millisekunden</td><td>2340</td></tr>
<tr><td><code>eval_cost_usd</code></td><td>Pro Span</td><td>USD</td><td>0.0087</td></tr>
<tr><td><code>eval_pass_rate</code></td><td>Trace (Gesamt)</td><td>0.0 ‚Äì 1.0</td><td>0.875 = 14/16 bestanden</td></tr>
<tr><td><code>eval_total_cost_usd</code></td><td>Trace (Gesamt)</td><td>USD</td><td>0.1392</td></tr>
<tr><td><code>eval_avg_latency_ms</code></td><td>Trace (Gesamt)</td><td>Millisekunden</td><td>2150</td></tr>
</table>


<!-- ================================================================== -->
<h2 id="production">6. Production-Request Durchlauf</h2>
<!-- ================================================================== -->

<div class="diagram-container">
<pre class="mermaid">
sequenceDiagram
    participant Client as GraphQL Client
    participant API as VEEDS API<br/>(Spring Boot)
    participant PR as proofreadEntry()
    participant LF as Langfuse SDK
    participant LFWEB as langfuse-web
    participant BED as AWS Bedrock<br/>(Claude 3.5 Sonnet)

    Client->>API: mutation proofreadYamlEntry(input)
    API->>PR: proofreadEntry(yamlEntry, options)

    Note over PR,LF: Schritt 1: Prompt laden
    PR->>LF: trace.span("load-prompt")
    PR->>LFWEB: getPrompt("veeds-proofreader",<br/>label: "production")
    alt Langfuse erreichbar
        LFWEB-->>PR: Prompt v3 (cached 5min)
        PR->>PR: prompt.compile({ yaml_entry })
    else Langfuse offline
        PR->>PR: Fallback: eval/prompt.txt
    end

    Note over PR,BED: Schritt 2: Bedrock (mit Retry)
    PR->>LF: trace.generation("bedrock-claude")
    loop Max 3 Retries
        PR->>BED: InvokeModelCommand
        alt ThrottlingException
            BED-->>PR: 429 Too Many Requests
            PR->>PR: wait(1s √ó 2^attempt + jitter)
        else Erfolg
            BED-->>PR: Response + Token Usage
        end
    end
    PR->>LF: generation.end(output, usage)

    Note over PR: Schritt 3: Response parsen
    PR->>LF: trace.span("parse-response")
    PR->>PR: Regex JSON-Extraktion<br/>JSON.parse()
    PR->>LF: trace.score("processing_time_ms", 2340)
    PR->>LF: trace.update(output: result)

    PR-->>API: ProofreadResult
    API-->>Client: { data: { proofreadYamlEntry: { ... } } }

    Note over LF,LFWEB: Async (im Hintergrund)
    LF-->>LFWEB: POST /api/public/ingestion<br/>(batched Events)
</pre>
<p class="diagram-label">Abb. 9: Vollst√§ndiger Production-Request mit Tracing und Retry</p>
</div>


<!-- ================================================================== -->
<h2 id="gitlab">7. GitLab CI/CD Pipeline im Detail</h2>
<!-- ================================================================== -->

<h3>Pipeline-√úbersicht</h3>

<div class="diagram-container">
<pre class="mermaid">
graph LR
    subgraph "Stage: quality"
        PFE["<b>promptfoo-eval</b><br/><span class='badge badge-red'>blocking</span><br/>MR + Main + Nightly"]
        PFC["<b>promptfoo-compare</b><br/><span class='badge badge-green'>optional</span><br/>Nur Nightly"]
    end

    subgraph "Stage: performance"
        K6L["<b>k6-load-test</b><br/><span class='badge badge-orange'>required</span><br/>Main + Nightly"]
        K6S["<b>k6-stress-test</b><br/><span class='badge badge-green'>optional</span><br/>Nur Nightly"]
    end

    subgraph "Stage: report"
        SUM["<b>pipeline-summary</b><br/>Main + Nightly"]
    end

    PFE -->|"needs"| K6L
    K6L --> SUM
    PFC --> SUM
    K6S --> SUM

    style PFE fill:#2e1618,stroke:#f06060,color:#e2e4eb
    style K6L fill:#221c14,stroke:#f0a050,color:#e2e4eb
    style PFC fill:#152220,stroke:#58d5a8,color:#e2e4eb
    style K6S fill:#152220,stroke:#58d5a8,color:#e2e4eb
    style SUM fill:#1a2a4a,stroke:#6c8cff,color:#e2e4eb
</pre>
<p class="diagram-label">Abb. 10: GitLab Pipeline-Architektur ‚Äî 3 Stages</p>
</div>

<h3>Detaillierter Ablauf: promptfoo-eval Job</h3>

<p>Dieser Job ist der <strong>Quality Gate</strong> ‚Äî er blockiert den Merge Request, wenn Tests fehlschlagen:</p>

<div class="diagram-container">
<pre class="mermaid">
sequenceDiagram
    participant GL as GitLab Runner
    participant NODE as node:20-slim Container
    participant GD as golden_dataset.json
    participant GEN as generate-promptfoo-tests.ts
    participant PF as Promptfoo CLI
    participant BED as AWS Bedrock
    participant PS as push-scores-to-langfuse.ts
    participant LF as Langfuse

    GL->>NODE: docker run node:20-slim

    Note over NODE: Phase 1: Setup
    NODE->>NODE: npm ci --prefer-offline

    Note over NODE,GD: Phase 2: Test-Generierung
    NODE->>GEN: npx tsx eval/generate-promptfoo-tests.ts
    GEN->>GD: Liest golden_dataset.json (16 Test Cases)
    GEN->>GEN: Pro Test Case: Assertions generieren
    GEN->>NODE: Schreibt eval/generated-tests.yaml

    Note over NODE,BED: Phase 3: Evaluation
    NODE->>PF: npx promptfoo eval --assert
    PF->>PF: L√§dt promptfooconfig.yaml
    PF->>PF: L√§dt generated-tests.yaml (16 Tests)
    PF->>PF: L√§dt eval/prompt.txt (Prompt Template)

    loop 16 Test Cases (maxConcurrency: 5)
        PF->>PF: Setzt yaml_entry in Prompt ein
        PF->>BED: Bedrock InvokeModel
        BED-->>PF: LLM Response
        PF->>PF: Assertions pr√ºfen:<br/>is-json ‚úì latency ‚úì cost ‚úì<br/>javascript checks ‚úì/‚úó<br/>g-eval (LLM-as-Judge) ‚úì/‚úó
    end

    PF->>NODE: Exit 0 (alle bestanden) oder Exit 1 (Fehler)
    PF->>NODE: Schreibt eval/results/ci-{pipeline}.json

    Note over NODE,LF: Phase 4: Score-Push (optional)
    NODE->>PS: npx tsx scripts/push-scores-to-langfuse.ts
    PS->>PS: Liest ci-{pipeline}.json
    PS->>LF: Trace "promptfoo-evaluation"<br/>+ 16 Spans + Scores
    PS->>LF: Aggregate: pass_rate, cost, latency

    Note over GL: Phase 5: Artefakte
    NODE-->>GL: eval/results/ci-{pipeline}.json<br/>(aufbewahrt 30 Tage)

    alt Exit 0
        GL->>GL: ‚úÖ Job passed ‚Üí MR kann gemerged werden
    else Exit 1
        GL->>GL: ‚ùå Job failed ‚Üí MR blockiert
    end
</pre>
<p class="diagram-label">Abb. 11: promptfoo-eval Job ‚Äî Schritt f√ºr Schritt</p>
</div>

<h3>Pipeline-Trigger-Matrix</h3>

<table>
<tr><th>Event</th><th>promptfoo-eval</th><th>promptfoo-compare</th><th>k6-load</th><th>k6-stress</th><th>summary</th></tr>
<tr><td>Merge Request erstellt/updated</td><td>‚úÖ <span class="badge badge-red">blocking</span></td><td>‚Äî</td><td>‚Äî</td><td>‚Äî</td><td>‚Äî</td></tr>
<tr><td>Push auf <code>main</code></td><td>‚úÖ <span class="badge badge-red">blocking</span></td><td>‚Äî</td><td>‚úÖ nach eval</td><td>‚Äî</td><td>‚úÖ</td></tr>
<tr><td>Nightly Schedule</td><td>‚úÖ <span class="badge badge-red">blocking</span></td><td>‚úÖ <span class="badge badge-green">optional</span></td><td>‚úÖ nach eval</td><td>‚úÖ <span class="badge badge-green">optional</span></td><td>‚úÖ</td></tr>
</table>

<div class="box box-green">
<strong>Warum der k6-Job von promptfoo-eval abh√§ngt</strong>
Der k6-Load-Test hat <code>needs: [promptfoo-eval]</code>. Dadurch wird sichergestellt, dass Lasttests nur laufen, wenn die Qualit√§t stimmt. Es macht keinen Sinn, 200 VUs gegen eine API zu feuern, die falsche Ergebnisse liefert.
</div>


<!-- ================================================================== -->
<h2 id="results">8. Wo landen die Ergebnisse?</h2>
<!-- ================================================================== -->

<div class="diagram-container">
<pre class="mermaid">
graph TB
    subgraph "üìÅ Dateisystem (GitLab Artefakte)"
        R1["eval/results/ci-{pipeline}.json<br/><i>Promptfoo Detailergebnisse</i><br/>30 Tage aufbewahrt"]
        R2["eval/results/compare-{pipeline}.json<br/><i>Modellvergleich Sonnet vs Haiku</i><br/>30 Tage aufbewahrt"]
        R3["k6-results.json<br/><i>k6 Performance-Daten</i><br/>‚Üí GitLab load_performance Report"]
    end

    subgraph "üü£ Langfuse Dashboard"
        L1["üìä Traces<br/><i>Production + Eval Runs</i>"]
        L2["üìà Scores<br/><i>pass_rate, cost, latency<br/>√ºber Zeit (Trend)</i>"]
        L3["üî¨ Experiments<br/><i>Golden Dataset vs Prompt v1/v2/v3</i>"]
        L4["üìù Prompts<br/><i>Versionshistorie mit Labels</i>"]
        L5["üìã Datasets<br/><i>16 Items mit Expected Output</i>"]
    end

    subgraph "üîµ GitLab UI"
        G1["MR: Load Performance Tab<br/><i>p95/p99 Vergleich</i>"]
        G2["Pipeline: Artefakt-Downloads"]
        G3["MR: Pipeline Status<br/><i>‚úÖ oder ‚ùå</i>"]
    end

    R3 --> G1
    R1 --> G2
    R2 --> G2

    style R1 fill:#152220,stroke:#58d5a8,color:#e2e4eb
    style R2 fill:#152220,stroke:#58d5a8,color:#e2e4eb
    style R3 fill:#221c14,stroke:#f0a050,color:#e2e4eb
    style L1 fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style L2 fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style L3 fill:#1c1628,stroke:#b07cf0,color:#e2e4eb
    style G1 fill:#1a2a4a,stroke:#6c8cff,color:#e2e4eb
    style G3 fill:#1a2a4a,stroke:#6c8cff,color:#e2e4eb
</pre>
<p class="diagram-label">Abb. 12: Ergebnis-Landkarte ‚Äî Wo man was findet</p>
</div>

<h3>Ergebnis-Referenz</h3>

<table>
<tr><th>Was suche ich?</th><th>Wo finde ich es?</th><th>Format</th></tr>
<tr><td>Ist mein MR-Prompt korrekt?</td><td>GitLab MR ‚Üí Pipeline Status</td><td>‚úÖ/‚ùå auf dem MR</td></tr>
<tr><td>Welche Tests sind fehlgeschlagen?</td><td>GitLab ‚Üí Pipeline ‚Üí Artefakt <code>ci-{id}.json</code></td><td>JSON mit Assertion-Details</td></tr>
<tr><td>Wie ist die Pass-Rate √ºber Zeit?</td><td>Langfuse ‚Üí Scores ‚Üí <code>eval_pass_rate</code> filtern</td><td>Score-Graph (Trend)</td></tr>
<tr><td>Was kostet ein Prompt pro Call?</td><td>Langfuse ‚Üí Traces ‚Üí Generation ‚Üí Usage</td><td>Input/Output Tokens + USD</td></tr>
<tr><td>Wie schnell ist die API unter Last?</td><td>GitLab MR ‚Üí Load Performance Tab</td><td>p95/p99 Vergleich</td></tr>
<tr><td>Sonnet vs. Haiku Qualit√§t?</td><td>Langfuse ‚Üí Datasets ‚Üí Experiments</td><td>Side-by-side Vergleich</td></tr>
<tr><td>Trace eines Production-Requests?</td><td>Langfuse ‚Üí Traces ‚Üí Filter by Tag/User</td><td>Waterfall mit Spans</td></tr>
<tr><td>Prompt-History & Label-Zuordnung?</td><td>Langfuse ‚Üí Prompts ‚Üí veeds-proofreader</td><td>Versionsliste</td></tr>
</table>


<!-- ================================================================== -->
<h2 id="improve">9. Verbesserungsvorschl√§ge</h2>
<!-- ================================================================== -->

<h3>üî¥ Hohe Priorit√§t</h3>

<h4>1. Headless Init (Zero-Click Setup)</h4>
<p>Die auskommentierte <code>LANGFUSE_INIT_*</code> Konfiguration in <code>docker-compose.yml</code> kann das manuelle Account-Erstellen eliminieren. Das <code>setup.sh</code> w√ºrde zus√§tzlich API Keys, Org und Projekt vorgenerieren. Dann geht der gesamte Flow ohne Browser:</p>
<pre><code>./setup.sh && docker compose up -d && npm run seed</code></pre>

<h4>2. dotenv in Scripts</h4>
<p>Aktuell: Die Scripts lesen <code>process.env</code>, aber laden die <code>.env</code> nicht automatisch. Man muss <code>source .env</code> vorher aufrufen oder ein Wrapper-Tool nutzen.</p>
<p>Fix: <code>import "dotenv/config"</code> als erste Zeile in jedem Script, oder <code>dotenv-cli</code> in package.json:</p>
<pre><code>"seed": "dotenv -- npx tsx scripts/seed-langfuse.ts"</code></pre>

<h4>3. Output Schema Validation</h4>
<p>Die JSON-Extraktion per Regex (<code>/\{[\s\S]*\}/</code>) ist fragil. Ein <code>ajv</code> JSON-Schema-Validator nach dem Parsing w√ºrde Halluzinationen sofort erkennen:</p>
<pre><code>const schema = {
  type: "object",
  required: ["isValid", "errors"],
  properties: {
    isValid: { type: "boolean" },
    errors: { type: "array", items: {
      required: ["field", "message", "severity"],
      properties: {
        field: { type: "string" },
        message: { type: "string" },
        severity: { enum: ["error", "warning", "info"] }
      }
    }}
  }
};</code></pre>

<h4>4. Structured Logging (pino)</h4>
<p>JSON-Logs mit automatischer Langfuse <code>traceId</code>-Korrelation f√ºr CloudWatch.</p>

<h3>üü† Mittlere Priorit√§t</h3>

<h4>5. Cost Alerting</h4>
<p>Nightly-Script, das die Langfuse Daily Metrics API abfragt. Alert bei Kosten > X/Tag oder > Y% Anstieg. Slack/Teams Webhook.</p>

<h4>6. Prompt A/B Testing in Production</h4>
<p>Feature-Flag (AppConfig) routet 10% des Traffics auf neue Prompt-Version. Langfuse-Trace hat Prompt-Version als Metadata ‚Üí im Dashboard nach Version filtern und Scores vergleichen.</p>

<h4>7. E2E Integration Test</h4>
<p>Automatischer Durchlauf: Docker Health ‚Üí Seed ‚Üí Proofreader Call ‚Üí Trace via Langfuse API verifizieren ‚Üí Promptfoo Eval ‚Üí Scores pushen ‚Üí Score verifizieren. Als <code>npm run test:integration</code>.</p>

<h4>8. k6 Testdaten aus Golden Dataset</h4>
<p>Die k6-Tests haben eigene hardcoded Testdaten. Ein Build-Step k√∂nnte <code>golden_dataset.json</code> in ein k6-kompatibles Format konvertieren ‚Üí auch hier Single Source of Truth.</p>

<h3>üü¢ Niedrige Priorit√§t / Langfristig</h3>

<h4>9. OpenTelemetry Integration</h4>
<p>Langfuse v3 hat OTLP-Endpoint. Wenn die Langfuse-Traces √ºber den OTel Collector geroutet w√ºrden, k√∂nntest du Spring Boot Application-Traces und LLM-Traces in einem Backend zusammenf√ºhren.</p>

<h4>10. Multi-Model Routing</h4>
<p>Einfache True-Positive-F√§lle (offensichtlich falsche materialNumber) an Haiku statt Sonnet ‚Üí ~10x g√ºnstiger bei gleicher Qualit√§t. Router basierend auf Input-Komplexit√§t.</p>

<h4>11. Circuit Breaker</h4>
<p>Zus√§tzlich zum Retry: Nach N aufeinanderfolgenden Fehlern sofort mit Fallback antworten statt Bedrock weiter zu bombardieren (<code>cockatiel</code> oder <code>opossum</code> Library).</p>

<h4>12. Human-in-the-Loop (Langfuse Annotations)</h4>
<p>Traces mit niedriger Confidence ‚Üí Langfuse Annotation Queue ‚Üí Domain-Experte reviewed ‚Üí Score flie√üt als Ground Truth ins Golden Dataset zur√ºck. Schlie√üt den Feedback-Loop.</p>

<script>
mermaid.initialize({
  startOnLoad: true,
  theme: 'dark',
  themeVariables: {
    primaryColor: '#1c1628',
    primaryTextColor: '#e2e4eb',
    primaryBorderColor: '#b07cf0',
    lineColor: '#6c8cff',
    secondaryColor: '#152220',
    tertiaryColor: '#221c14',
    noteBkgColor: '#232734',
    noteTextColor: '#e2e4eb',
    noteBorderColor: '#2e3347',
    actorBkg: '#1a2a4a',
    actorTextColor: '#e2e4eb',
    actorBorder: '#6c8cff',
    signalColor: '#8b90a0',
    labelBoxBkgColor: '#232734',
    labelTextColor: '#e2e4eb',
    sectionBkgColor: '#1a1d27',
    altSectionBkgColor: '#232734',
    taskBkgColor: '#1c1628',
    taskBorderColor: '#b07cf0',
    taskTextColor: '#e2e4eb',
    taskTextLightColor: '#e2e4eb',
    doneTaskBkgColor: '#152220',
    activeTaskBkgColor: '#1a2a4a',
    gridColor: '#2e3347',
    todayLineColor: '#f0a050',
  },
  flowchart: { curve: 'basis', padding: 15 },
  sequence: { mirrorActors: false, messageMargin: 40 },
  gantt: { barHeight: 25, fontSize: 12 }
});
</script>

</body>
</html>
